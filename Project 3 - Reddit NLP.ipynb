{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Project 3 - Reddit NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/reddit.png\" alt=\"reddit\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Reddit, the user-generated internet news/content aggregator has at it's heart a system of creating specialized \"subreddits\" where users may gather and discuss pertinent interests together. Our goal is to design a machine-learning algorithm based on supervised Natural Language Processing (NLP) and see if it has the ability to distinguish whether a post came from one of two selected subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit is a community-based forum dubbed often as \"The frontage of the internet.\" Reddit has a central front-page, where it displays the top posts from many subreddits. These subreddits, are smaller, subdivided into sub-forums specific to topics, which are chosen by the members of reddit. Subreddits can be created by individual members and are typically self-moderated unless they violate the Reddit terms of service. Users can post their own content as well as repy to other's posts with comments. By utilizing the posts and comments system, we believe we can use supervised Natural Language Processing (NLP) to predict whether a given post came from one subreddit or another of our choosing. By training on these two subreddits of our choosing, we can possible reveal some intrcacies and trends that that may underly why some people post to one reddit or another. If our NLP can achieve this result with greater precision than a baseline model, we can walk away with some great insights.\n",
    "\n",
    "\n",
    "   ### Contents:\n",
    "\n",
    "   - [Imports](#Imports)\n",
    "   - [Read-in Data Files](#Read-in-Data-Files)\n",
    "   - [Preprocessing](#Preprocessing)\n",
    "   - [Functions](#Functions)\n",
    "   - [Exploratory Data Analysis](#Exploratory-Data-Analysis-(EDA))\n",
    "   - [Modeling](#Modeling)\n",
    "   - [Conclusions and Recommendations](#Conclusions-and-Recommendations)\n",
    "   - [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:48.993973Z",
     "start_time": "2019-04-08T14:41:45.343704Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time, requests, json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Test Imports - May not be used.\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "#538 Style graphs\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "#Optimized graphs for retina displays\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#Random State\n",
    "np.random.seed(41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Pushshift API Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.015556Z",
     "start_time": "2019-04-08T14:41:48.998036Z"
    }
   },
   "outputs": [],
   "source": [
    "#Pushshift Query provided by Brian Collins\n",
    "def query_pushshift(subreddit, kind='comment', skip=30, times=6, \n",
    "                    subfield = ['title', 'selftext', 'subreddit', 'created_utc', 'author', \n",
    "                                'num_comments', 'score', 'is_self'],\n",
    "                    comfields = ['body', 'score', 'created_utc']):\n",
    "\n",
    "    stem = \"https://api.pushshift.io/reddit/search/{}/?subreddit={}&size=500\".format(kind, subreddit)\n",
    "    mylist = []\n",
    "    \n",
    "    for x in range(1, times):\n",
    "        \n",
    "        URL = \"{}&after={}d\".format(stem, skip * x)\n",
    "        print(URL)\n",
    "        response = requests.get(URL)\n",
    "        assert response.status_code == 200, \"Link Dead\"\n",
    "        mine = response.json()['data']\n",
    "        df = pd.DataFrame.from_dict(mine)\n",
    "        mylist.append(df)\n",
    "        time.sleep(2)\n",
    "        \n",
    "    full = pd.concat(mylist, sort=False)\n",
    "    \n",
    "    if kind == \"submission\":\n",
    "        \n",
    "        full = full[subfield]\n",
    "        \n",
    "        full = full.drop_duplicates()\n",
    "        \n",
    "        full = full.loc[full['is_self'] == True]\n",
    "        \n",
    "    def get_date(created):\n",
    "        return dt.date.fromtimestamp(created)\n",
    "    \n",
    "    _timestamp = full[\"created_utc\"].apply(get_date)\n",
    "    \n",
    "    full['timestamp'] = _timestamp\n",
    "\n",
    "    print(full.shape)\n",
    "    \n",
    "    return full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T07:57:08.018398Z",
     "start_time": "2019-04-08T07:57:08.006520Z"
    }
   },
   "source": [
    "## Initial Scrape Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.022969Z",
     "start_time": "2019-04-08T14:41:49.018857Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Chosen Subreddits\n",
    "# s1 = 'legaladvice'\n",
    "# s2 = 'relationship_advice'\n",
    "\n",
    "# # #API Calls for data\n",
    "# s1_comment = query_pushshift(s1, kind='comment', times=24)\n",
    "# s1_submission = query_pushshift(s1, kind='submission', times=24)\n",
    "# s2_comment = query_pushshift(s2, kind='comment', times=24)\n",
    "# s2_submission = query_pushshift(s2, kind='submission', times=24)\n",
    "\n",
    "# #Save API data to files for quick access later\n",
    "# s1_comment.to_csv('./data/s1_comment.csv', index=False)\n",
    "# s1_submission.to_csv('./data/s1_submission.csv', index=False)\n",
    "# s2_comment.to_csv('./data/s2_comment.csv', index=False)\n",
    "# s2_submission.to_csv('./data/s2_submission.csv', index=False)\n",
    "\n",
    "#Uncomment out if rescraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "For this study, we used the r/legaladvice and r/relationship_advice subreddits. r/legaladvice typically deals with members discussing legal discussion whether it's theoretical discussion or getting actual advice on what to do pertaining to court cases they may be involved with. Lawyers who are also redditors also typically chime in with advice. r/relationship_advice pertains with people coming with relationoship issues and soliciting other members to weigh in on their topic. We believed these two forums would initially be hard to distinguish because a vast majority of times on r/relationship_advice, the questions pertain to divorce, getting a lawyer, and legal options given a bad breakup. Going forth, r/legaladvice will be \"s1\" (subreddit 1) and r/relationship_advice will be \"s2\" (subreddit 2) as they pertain to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.804908Z",
     "start_time": "2019-04-08T14:41:49.028471Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read in saved data from API calls\n",
    "s1_comment = pd.read_csv('./data/s1_comment.csv')\n",
    "s1_submission = pd.read_csv('./data/s1_submission.csv')\n",
    "s2_comment = pd.read_csv('./data/s2_comment.csv')\n",
    "s2_submission = pd.read_csv('./data/s2_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.817404Z",
     "start_time": "2019-04-08T14:41:49.809745Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11500, 35), (11500, 33), (11498, 9), (11454, 9))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check dimensionality of queried data. Our initial goal was to use as to 10,000 as possible for each subreddit.\n",
    "s1_comment.shape, s2_comment.shape, s1_submission.shape, s2_submission.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "We have around 11500 results from each query, if the EDA does not require dropping many columns, it is very unlikely we will need to deal with unbalanced classes. Part of the Pushshift query filtered out titles without selftext as well, which is often a common occurrence with reddit as many posts are simple pictures with no text content to actually analyze. This script omits those and only grabs pertinent posts with selftexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Columns Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.829268Z",
     "start_time": "2019-04-08T14:41:49.821203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'selftext', 'subreddit', 'created_utc', 'author',\n",
       "       'num_comments', 'score', 'is_self', 'timestamp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Discover columns contained for each submission group\n",
    "s1_submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.845039Z",
     "start_time": "2019-04-08T14:41:49.832427Z"
    }
   },
   "outputs": [],
   "source": [
    "#Remove data columns we will no use\n",
    "s1_submission.drop(columns=['created_utc', 'author', 'num_comments', \n",
    "                            'subreddit', 'is_self', 'timestamp'], inplace=True)\n",
    "s2_submission.drop(columns=['created_utc', 'author', 'num_comments', \n",
    "                            'subreddit', 'is_self', 'timestamp'], inplace=True)\n",
    "\n",
    "#Subreddit #1 is a 0 and subreddit #2 is a 1\n",
    "s1_submission['subreddit'] = 0\n",
    "s2_submission['subreddit'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.854384Z",
     "start_time": "2019-04-08T14:41:49.848738Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11498, 4), (11454, 4))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_submission.shape, s2_submission.shape #Shape check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Assigning classification weight to subreddits with s1 being 0 and s2 being a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.884046Z",
     "start_time": "2019-04-08T14:41:49.857995Z"
    }
   },
   "outputs": [],
   "source": [
    "#Remove [deleted] and [removed] text self texts, and only took scores greater than 0 (1 or above).\n",
    "#We believe that posts that score higher will typically be more aligned with the belief and\n",
    "#culture of each subreddit\n",
    "s1_submission = s1_submission[(s1_submission['selftext'] != '[deleted]') & \n",
    "                              (s1_submission['selftext'] != '[removed]') & \n",
    "                              (s1_submission['score'] > 0)]\n",
    "\n",
    "s2_submission = s2_submission[(s2_submission['selftext'] != '[deleted]') & \n",
    "                              (s2_submission['selftext'] != '[removed]') & \n",
    "                              (s2_submission['score'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.891919Z",
     "start_time": "2019-04-08T14:41:49.886085Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 4), (8685, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_submission.shape, s2_submission.shape #Shape check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only submissions with score > 0. Submissions which score higher (more highly-rated) by the community are defintely an indicator of a subreddit's tone or style. Moderators will also remove ones that are not favorable or score poorly as well. We only took a score of 0 as our baseline, as scores can be negative, indicating negative sentiment from those who view the subreddit. We also take the assumption that any random post at it's extremes with no matching words from a subreddit should match just as poorly in any other subreddit and will be at best a random choice anyway. We do not want our model to match on these \"random\" submissions, which if anything, only add noise. We also remove posts that have \"removed\" or \"deleted\" in their text, as the submittor decided to remove the content after posting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T09:32:31.790068Z",
     "start_time": "2019-04-08T09:32:31.613894Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#We no longer need scores after we have filtered, so let's get rid of them.\n",
    "s1_submission.drop(columns=['score'], inplace=True)\n",
    "s2_submission.drop(columns=['score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.902465Z",
     "start_time": "2019-04-08T14:41:49.894475Z"
    }
   },
   "outputs": [],
   "source": [
    "tot_sub = pd.concat([s1_submission, s2_submission]) #Assemble final dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.910697Z",
     "start_time": "2019-04-08T14:41:49.904770Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16298, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_sub.shape #Confirm shape of final analysis dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.927213Z",
     "start_time": "2019-04-08T14:41:49.913569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Workmans comp question</td>\n",
       "      <td>Hi,\\nI really do not know a lot about this. La...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Domestic violence and how to approach it? (Texas)</td>\n",
       "      <td>My father has always been physically and verba...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Normally at my work, we are not allowed any br...</td>\n",
       "      <td>I'm a minor from the state of New York. I work...</td>\n",
       "      <td>498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FL- Spouse passed with no will, what to do abo...</td>\n",
       "      <td>My husband recently passed away 2 months ago u...</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>evicting roommate</td>\n",
       "      <td>living in Georgia, i have a monthly rental agr...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                             Workmans comp question   \n",
       "1  Domestic violence and how to approach it? (Texas)   \n",
       "4  Normally at my work, we are not allowed any br...   \n",
       "5  FL- Spouse passed with no will, what to do abo...   \n",
       "7                                  evicting roommate   \n",
       "\n",
       "                                            selftext  score  subreddit  \n",
       "0  Hi,\\nI really do not know a lot about this. La...      6          0  \n",
       "1  My father has always been physically and verba...      6          0  \n",
       "4  I'm a minor from the state of New York. I work...    498          0  \n",
       "5  My husband recently passed away 2 months ago u...     50          0  \n",
       "7  living in Georgia, i have a monthly rental agr...      2          0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final dataframe for submissions (posts) gives us both title and selftext columns to work with should we choose either or both to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment Column Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.937231Z",
     "start_time": "2019-04-08T14:41:49.930309Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11500, 35), (11500, 33))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_comment.shape, s2_comment.shape #Seems s1 has some extra data, we drop all the columns except body and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.948232Z",
     "start_time": "2019-04-08T14:41:49.940012Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['author', 'author_flair_background_color', 'author_flair_css_class',\n",
       "        'author_flair_richtext', 'author_flair_template_id',\n",
       "        'author_flair_text', 'author_flair_text_color', 'author_flair_type',\n",
       "        'author_fullname', 'author_patreon_flair', 'body', 'created_utc',\n",
       "        'distinguished', 'gildings', 'id', 'link_id', 'no_follow', 'parent_id',\n",
       "        'permalink', 'retrieved_on', 'score', 'send_replies', 'stickied',\n",
       "        'subreddit', 'subreddit_id', 'author_cakeday', 'rte_mode', 'can_gild',\n",
       "        'collapsed', 'collapsed_reason', 'controversiality', 'edited', 'gilded',\n",
       "        'is_submitter', 'timestamp'],\n",
       "       dtype='object'),\n",
       " Index(['author', 'author_cakeday', 'author_flair_background_color',\n",
       "        'author_flair_css_class', 'author_flair_richtext',\n",
       "        'author_flair_template_id', 'author_flair_text',\n",
       "        'author_flair_text_color', 'author_flair_type', 'author_fullname',\n",
       "        'author_patreon_flair', 'body', 'created_utc', 'gildings', 'id',\n",
       "        'link_id', 'no_follow', 'parent_id', 'permalink', 'retrieved_on',\n",
       "        'score', 'send_replies', 'stickied', 'subreddit', 'subreddit_id',\n",
       "        'rte_mode', 'can_gild', 'collapsed', 'collapsed_reason',\n",
       "        'controversiality', 'edited', 'is_submitter', 'timestamp'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_comment.columns, s2_comment.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Interestingly enough, even though the scrape proceeded in the same way, the s2 has extra columns, these might be extra columns from that particular subreddit only. Because it only appears in one, we will remove it since we have no basis of comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.968771Z",
     "start_time": "2019-04-08T14:41:49.951148Z"
    }
   },
   "outputs": [],
   "source": [
    "#Drop columns we will not use.\n",
    "s1_comment.drop(columns=['author', 'author_flair_background_color', 'author_flair_css_class',\n",
    "       'author_flair_richtext', 'author_flair_template_id',\n",
    "       'author_flair_text', 'author_flair_text_color', 'author_flair_type',\n",
    "       'author_fullname', 'author_patreon_flair', 'created_utc',\n",
    "       'distinguished', 'gildings', 'id', 'link_id', 'no_follow', 'parent_id',\n",
    "       'permalink', 'retrieved_on', 'send_replies', 'stickied',\n",
    "       'subreddit', 'subreddit_id', 'author_cakeday', 'rte_mode', 'can_gild',\n",
    "       'collapsed', 'collapsed_reason', 'controversiality', 'edited', 'gilded',\n",
    "       'is_submitter', 'timestamp'], inplace=True)\n",
    "\n",
    "s2_comment.drop(columns=['author', 'author_cakeday', 'author_flair_background_color',\n",
    "       'author_flair_css_class', 'author_flair_richtext',\n",
    "       'author_flair_template_id', 'author_flair_text',\n",
    "       'author_flair_text_color', 'author_flair_type', 'author_fullname',\n",
    "       'author_patreon_flair', 'created_utc', 'gildings', 'id',\n",
    "       'link_id', 'no_follow', 'parent_id', 'permalink', 'retrieved_on', \n",
    "        'send_replies', 'stickied', 'subreddit', 'subreddit_id',\n",
    "       'rte_mode', 'can_gild', 'collapsed', 'collapsed_reason',\n",
    "       'controversiality', 'edited', 'is_submitter', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:49.977774Z",
     "start_time": "2019-04-08T14:41:49.971343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11500, 2), (11500, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_comment.shape, s2_comment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.014359Z",
     "start_time": "2019-04-08T14:41:49.988820Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Remove [deleted] and [removed] text self texts, and only took scores greater than 0 (1 or above).\n",
    "#We believe that posts that score higher will typically be more aligned with the belief and\n",
    "#culture of each subreddit\n",
    "s1_comment = s1_comment[(s1_comment['body'] != '[deleted]') & \n",
    "                              (s1_comment['body'] != '[removed]') & \n",
    "                              (s1_comment['score'] > 0)]\n",
    "\n",
    "s2_comment = s2_comment[(s2_comment['body'] != '[deleted]') & \n",
    "                              (s2_comment['body'] != '[removed]') & \n",
    "                              (s2_comment['score'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.026647Z",
     "start_time": "2019-04-08T14:41:50.020246Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10388, 2), (10550, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_comment.shape, s2_comment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "We do the same thing we did for submissions, dropping rows with a score of 0, which people who browse the subreddit chose to vote less favorably, such that it does not align with their views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.151147Z",
     "start_time": "2019-04-08T14:41:50.030026Z"
    }
   },
   "outputs": [],
   "source": [
    "#We no longer need scores after we have filtered, so let's get rid of them.\n",
    "s1_comment.drop(columns=['score'], inplace=True)\n",
    "s2_comment.drop(columns=['score'], inplace=True)\n",
    "\n",
    "#Add column to score which subreddit it came from 0 for s1 and 1 for s2\n",
    "s1_comment['subreddit'] = 0\n",
    "s2_comment['subreddit'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Assign the same weighting to the classifier. 0 for s1 and 1 for s2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.162135Z",
     "start_time": "2019-04-08T14:41:50.153804Z"
    }
   },
   "outputs": [],
   "source": [
    "tot_com = pd.concat([s1_comment, s2_comment]) #Assemble final dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.171617Z",
     "start_time": "2019-04-08T14:41:50.164284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20938, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.188159Z",
     "start_time": "2019-04-08T14:41:50.175664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you. I suppose if anything were to happe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The building previously allowed smoking inside...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That's true. People get slipped discs all the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does pulling your product violate that contrac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is she subleasing from? Are you and your o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  subreddit\n",
       "0  Thank you. I suppose if anything were to happe...          0\n",
       "1  The building previously allowed smoking inside...          0\n",
       "2  That's true. People get slipped discs all the ...          0\n",
       "3  Does pulling your product violate that contrac...          0\n",
       "4  Who is she subleasing from? Are you and your o...          0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.203888Z",
     "start_time": "2019-04-08T14:41:50.192101Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body         0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_com.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.221810Z",
     "start_time": "2019-04-08T14:41:50.207296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "selftext     208\n",
       "score          0\n",
       "subreddit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_sub.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.241833Z",
     "start_time": "2019-04-08T14:41:50.225193Z"
    }
   },
   "outputs": [],
   "source": [
    "tot_sub.dropna(inplace=True) #Dropped some nulls in submissions. Comments did not have nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.459909Z",
     "start_time": "2019-04-08T14:41:50.245007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.534431\n",
       "0    0.465569\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_sub.subreddit.value_counts(normalize=True) #Proportions are still relatively close, no need to balance classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.482143Z",
     "start_time": "2019-04-08T14:41:50.462946Z"
    }
   },
   "outputs": [],
   "source": [
    "#Train-Test-Split\n",
    "X = tot_com['body']\n",
    "y = tot_com['subreddit']\n",
    "X_train_com, X_test_com, y_train_com, y_test_com = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.490750Z",
     "start_time": "2019-04-08T14:41:50.484290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15703,), (5235,), (15703,), (5235,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_com.shape, X_test_com.shape, y_train_com.shape, y_test_com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.510949Z",
     "start_time": "2019-04-08T14:41:50.494057Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train-Test-Split Submission Data\n",
    "X = tot_sub['selftext']\n",
    "y = tot_sub['subreddit']\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.520291Z",
     "start_time": "2019-04-08T14:41:50.513073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12067,), (4023,), (12067,), (4023,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sub.shape, X_test_sub.shape, y_train_sub.shape, y_test_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniers needed to be objects to passed into any Vectorizer. Define custom classes for this to happen. Classes taken from StackOverflow - https://stackoverflow.com/questions/47423854/sklearn-adding-lemmatizer-to-countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.529517Z",
     "start_time": "2019-04-08T14:41:50.523354Z"
    }
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.538140Z",
     "start_time": "2019-04-08T14:41:50.532548Z"
    }
   },
   "outputs": [],
   "source": [
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.stem(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:41:50.548890Z",
     "start_time": "2019-04-08T14:41:50.541994Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([\n",
    "    ('cv', CountVectorizer(tokenizer=StemTokenizer())), #interchange tokenizer with LemmaTokenizer\n",
    "    ('lr', LogisticRegression(n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cv__stop_words' : ['english'],\n",
    "    'cv__max_features': [3000],\n",
    "    'cv__min_df': [2],\n",
    "    'cv__max_df': [.9],\n",
    "    'cv__ngram_range': [(1,2)] #Tried (1,3) - (1,2) performed better\n",
    "#     'lr__solver' : ['newton-cg']\n",
    "#     'lr__penalty' : ['l2'],\n",
    "#     'lr__C': [.1, .5]\n",
    "}\n",
    "\n",
    "gs_lr = GridSearchCV(pipe_lr, param_grid=pipe_params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:54:29.805035Z",
     "start_time": "2019-04-08T14:41:50.551831Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_a...penalty='l2', random_state=None, solver='warn', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'cv__stop_words': ['english'], 'cv__max_features': [3000], 'cv__min_df': [2], 'cv__max_df': [0.9], 'cv__ngram_range': [(1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr.fit(X_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:58:07.240787Z",
     "start_time": "2019-04-08T14:54:29.808214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9985911991381453\n",
      "0.964951528709918\n",
      "0.800611348150035\n",
      "0.7910219675262655\n"
     ]
    }
   ],
   "source": [
    "gs_lr.best_params_\n",
    "\n",
    "#Submission Scores\n",
    "print(gs_lr.score(X_train_sub, y_train_sub))\n",
    "print(gs_lr.score(X_test_sub, y_test_sub))\n",
    "\n",
    "#Comment Scores\n",
    "print(gs_lr.score(X_train_com, y_train_com))\n",
    "print(gs_lr.score(X_test_com, y_test_com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:58:07.261936Z",
     "start_time": "2019-04-08T14:58:07.245008Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_nb = Pipeline([\n",
    "    ('cv', CountVectorizer(tokenizer=StemTokenizer())),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cv__stop_words' : ['english'],\n",
    "    'cv__max_features': [3000],\n",
    "    'cv__min_df': [2],\n",
    "    'cv__max_df': [.9],\n",
    "    'cv__ngram_range': [(1,2)] #Tried (1,3) - (1,2) performed better\n",
    "#     'lr__solver' : ['newton-cg']\n",
    "#     'lr__penalty' : ['l2'],\n",
    "#     'lr__C': [.1, .5]\n",
    "}\n",
    "\n",
    "gs_nb = GridSearchCV(pipe_nb, param_grid=pipe_params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:12:04.011747Z",
     "start_time": "2019-04-08T14:58:07.272926Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_a...2b0>,\n",
       "        vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'cv__stop_words': ['english'], 'cv__max_features': [3000], 'cv__min_df': [2], 'cv__max_df': [0.9], 'cv__ngram_range': [(1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nb.fit(X_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:12:04.024253Z",
     "start_time": "2019-04-08T15:12:04.014810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv__max_df': 0.9,\n",
       " 'cv__max_features': 3000,\n",
       " 'cv__min_df': 2,\n",
       " 'cv__ngram_range': (1, 2),\n",
       " 'cv__stop_words': 'english'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:16:04.558956Z",
     "start_time": "2019-04-08T15:12:04.029320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842768897662867\n",
      "0.8324737344794652\n",
      "0.9636197895085771\n",
      "0.960974397216008\n"
     ]
    }
   ],
   "source": [
    "#Submission Scores\n",
    "print(gs_nb.score(X_train_com, y_train_com))\n",
    "print(gs_nb.score(X_test_com, y_test_com))\n",
    "\n",
    "#Comment Scores\n",
    "print(gs_nb.score(X_train_sub, y_train_sub))\n",
    "print(gs_nb.score(X_test_sub, y_test_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:16:04.587186Z",
     "start_time": "2019-04-08T15:16:04.566591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=3000, min_df=2,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        st...penalty='l2', random_state=None, solver='warn', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:16:04.647127Z",
     "start_time": "2019-04-08T15:16:04.596724Z"
    }
   },
   "outputs": [],
   "source": [
    "x = dict(zip(gs_lr.best_estimator_.named_steps['cv'].get_feature_names(), gs_lr.best_estimator_.named_steps['lr'].coef_[0] ))\n",
    "\n",
    "sorted_d = sorted(x.items(), key=lambda x: x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:16:04.655182Z",
     "start_time": "2019-04-08T15:16:04.650976Z"
    }
   },
   "outputs": [],
   "source": [
    "#Assemble Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:16:04.756750Z",
     "start_time": "2019-04-08T15:16:04.659251Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('legal', -2.5410729884401118),\n",
       " ('court', -1.663480042030627),\n",
       " ('troubl', -1.5423024078322232),\n",
       " ('landlord', -1.5314239702482597),\n",
       " ('lawyer', -1.4851860340607708),\n",
       " ('compani', -1.3453473636150088),\n",
       " ('charg', -1.321451231613138),\n",
       " ('illeg', -1.301273946388759),\n",
       " ('record', -1.2987724480409573),\n",
       " ('polic', -1.279128795893262),\n",
       " ('report', -1.2631030234430785),\n",
       " ('jail', -1.2413613387952447),\n",
       " ('california', -1.2251674434908113),\n",
       " ('case', -1.2218014891083153),\n",
       " ('process', -1.2083630960541671),\n",
       " ('insur', -1.1698043976548123),\n",
       " ('canada', -1.161317649956929),\n",
       " ('file', -1.1460168642305741),\n",
       " ('threaten', -1.145375194788915),\n",
       " ('texa', -1.144043530662481),\n",
       " ('. anyth', -1.1351777477266918),\n",
       " ('receiv', -1.0822341475860129),\n",
       " ('owner', -1.0792133366175631),\n",
       " ('cop', -1.046975706409726),\n",
       " ('state', -1.025691248737241),\n",
       " ('florida', -1.010546535680994),\n",
       " ('violat', -1.003321784431227),\n",
       " ('assum', -0.9899349848998493),\n",
       " (', thing', -0.9491264526488266),\n",
       " ('appli', -0.9318898756216875),\n",
       " ('contract', -0.926810633690237),\n",
       " ('sue', -0.9260733775835275),\n",
       " ('leas', -0.9210204017489007),\n",
       " ('websit', -0.9179542111436761),\n",
       " ('gun', -0.9145340593737881),\n",
       " ('allow', -0.8996280785407572),\n",
       " ('attorney', -0.8971232364776733),\n",
       " ('protect', -0.8912703000225827),\n",
       " ('permiss', -0.8766637952941763),\n",
       " ('harass', -0.862397615603039),\n",
       " ('paperwork', -0.8446367568736561),\n",
       " ('debt', -0.8403729718519255),\n",
       " ('titl', -0.8242638060041243),\n",
       " ('seek', -0.8219172543590497),\n",
       " ('custodi', -0.8164697183941388),\n",
       " ('anyth', -0.8003237878613513),\n",
       " ('law', -0.7972293705531746),\n",
       " ('option', -0.7940540919255569),\n",
       " ('inform', -0.7922110320775115),\n",
       " ('licens', -0.7917752512690844),\n",
       " ('transfer', -0.7912672734183067),\n",
       " ('bank', -0.790730398120695),\n",
       " ('like know', -0.7859133782663068),\n",
       " ('counti', -0.7817002933231624),\n",
       " ('pass', -0.7631404925949896),\n",
       " ('paid', -0.762210613570366),\n",
       " ('su', -0.7603789150099242),\n",
       " ('camera', -0.7583258638492739),\n",
       " ('notic', -0.7563219749738568),\n",
       " ('come .', -0.7461278690915611),\n",
       " ('drug', -0.7451198719152382),\n",
       " ('manag', -0.737830308503166),\n",
       " ('birth', -0.7372259102337567),\n",
       " ('confirm', -0.7326390544862136),\n",
       " ('cash', -0.7224601900552703),\n",
       " ('? )', -0.7146134269126783),\n",
       " ('certif', -0.7092811485028788),\n",
       " ('arrest', -0.7091700459279041),\n",
       " ('daughter', -0.7091461156197143),\n",
       " ('payment', -0.7080064104563836),\n",
       " ('kill', -0.7076590500860019),\n",
       " ('legal action', -0.7035996157453239),\n",
       " ('grandma', -0.7025319473379351),\n",
       " ('beat', -0.697769271924489),\n",
       " ('. alway', -0.6936116252037533),\n",
       " ('. ex', -0.6877265380218693),\n",
       " ('sign', -0.6851967419340796),\n",
       " ('locat', -0.6821579631014241),\n",
       " ('liabl', -0.6816973596321061),\n",
       " ('employe', -0.6814950298901736),\n",
       " ('info', -0.6795873406093296),\n",
       " ('end relationship', -0.6793462508228663),\n",
       " ('becaus ', -0.6778115155189205),\n",
       " ('live', -0.6709272485345854),\n",
       " ('12', -0.670911780674172),\n",
       " ('properti', -0.6681733005712349),\n",
       " ('ani', -0.6659136484249804),\n",
       " ('right ?', -0.6649754740282767),\n",
       " ('copi', -0.6643275855601003),\n",
       " ('steal', -0.6637025579161531),\n",
       " ('2 month', -0.6604430826705473),\n",
       " ('medic', -0.6596245643962245),\n",
       " ('threat', -0.6587803395415038),\n",
       " ('twice', -0.6578336920289378),\n",
       " ('thing like', -0.6478860699796694),\n",
       " ('edit :', -0.6472286334260655),\n",
       " ('driver', -0.642925266222375),\n",
       " ('matter', -0.6418037956422379),\n",
       " ('pain', -0.6413508716600522),\n",
       " (\"let 's\", -0.6390924148145541),\n",
       " ('investig', -0.6375313815046424),\n",
       " ('road', -0.6368126727862744),\n",
       " ('hey', -0.635029978306017),\n",
       " ('worker', -0.6348722733414088),\n",
       " ('possess', -0.6347883178881322),\n",
       " ('claim', -0.632813321688459),\n",
       " ('neighbor', -0.6319367249040576),\n",
       " ('includ', -0.6274776767684377),\n",
       " ('. option', -0.624215450674539),\n",
       " ('boss', -0.6233399772326157),\n",
       " ('employ', -0.6157673558856177),\n",
       " ('onlin', -0.6151238146242473),\n",
       " ('taken', -0.6127704848036188),\n",
       " ('ticket', -0.612378813873528),\n",
       " ('stupid', -0.6118093250478213),\n",
       " ('sold', -0.6088682256693727),\n",
       " ('defend', -0.6030231404191498),\n",
       " ('old', -0.6017593427343069),\n",
       " ('... .', -0.6003510551016736),\n",
       " ('replac', -0.5981238240303144),\n",
       " ('divorc', -0.5961983296250464),\n",
       " ('minor', -0.59471781284734),\n",
       " ('suspect', -0.5946493712656649),\n",
       " ('insur .', -0.5921090375893833),\n",
       " ('limit', -0.5910363034480457),\n",
       " ('servic', -0.5903682749363842),\n",
       " ('order', -0.5860992463654573),\n",
       " ('question', -0.5850148773323806),\n",
       " ('option ?', -0.5826086892960654),\n",
       " ('offic', -0.5802475651913195),\n",
       " ('stolen', -0.5798010466722846),\n",
       " ('deposit', -0.5783288161686643),\n",
       " ('prove', -0.5767162355165453),\n",
       " ('split', -0.574873672399864),\n",
       " ('$', -0.5698082147507637),\n",
       " ('financ', -0.5693841122239671),\n",
       " ('press', -0.5685872097414858),\n",
       " ('instal', -0.5683561440170439),\n",
       " ('paper', -0.5616420753751735),\n",
       " ('requir', -0.56078996253107),\n",
       " ('action', -0.5603427047491495),\n",
       " ('power', -0.5602410170640983),\n",
       " ('set', -0.5585697772159413),\n",
       " ('. left', -0.5578971272954594),\n",
       " (', say', -0.5556610048295831),\n",
       " (', ani', -0.5554836863675109),\n",
       " ('tenant', -0.552588351773049),\n",
       " ('situat .', -0.5501778199215966),\n",
       " ('use', -0.5492121616517015),\n",
       " ('paranoid', -0.5489473498846874),\n",
       " ('pass away', -0.548665413092729),\n",
       " ('hire', -0.5448522395528695),\n",
       " ('drunk', -0.5438689384771307),\n",
       " ('govern', -0.5437533860802747),\n",
       " ('18', -0.5426340459822888),\n",
       " ('pleas help', -0.5413689904534046),\n",
       " ('speed', -0.5401406312435122),\n",
       " ('bar', -0.5399760145719623),\n",
       " ('lead', -0.5399264752024356),\n",
       " ('? thank', -0.5396253451380241),\n",
       " ('appoint', -0.5357575861314439),\n",
       " ('mark', -0.5347693752655647),\n",
       " ('knowledg', -0.5337236532542432),\n",
       " ('countri', -0.5322351254201776),\n",
       " ('feel better', -0.5261432842768458),\n",
       " (', ca', -0.5246982085718565),\n",
       " ('recours', -0.5237877608262531),\n",
       " ('lie .', -0.5226139036895804),\n",
       " ('chanc', -0.5219665053646506),\n",
       " ('violent', -0.5208377161527848),\n",
       " ('air', -0.5205367742019682),\n",
       " ('school', -0.5204516213922913),\n",
       " ('regist', -0.518263188333367),\n",
       " (\", 've\", -0.5171329632679281),\n",
       " ('mail', -0.5150947502732423),\n",
       " (', work', -0.5145529339337772),\n",
       " ('enter', -0.5133550247974253),\n",
       " ('directli', -0.5121536245564813),\n",
       " (', live', -0.5079744142900587),\n",
       " ('#', -0.5054716259031713),\n",
       " ('sell', -0.5054466695552136),\n",
       " ('fear', -0.5053343889910412),\n",
       " ('judg', -0.5049784250647894),\n",
       " ('help .', -0.5023951318988565),\n",
       " ('19', -0.5008572942180451),\n",
       " ('horribl', -0.500339758971264),\n",
       " ('. idea', -0.4998503627776583),\n",
       " ('document', -0.49925152508411047),\n",
       " ('( http', -0.4991846063283775),\n",
       " ('creat', -0.4984461808884375),\n",
       " ('straight', -0.49791983576326354),\n",
       " ('problem .', -0.49475690771859565),\n",
       " ('step', -0.4945882721191343),\n",
       " ('year old', -0.4944409131404549),\n",
       " ('prior', -0.4942863544066256),\n",
       " ('t realli', -0.49364471837560564),\n",
       " ('state .', -0.4921098318126765),\n",
       " ('hi mother', -0.49188795029612237),\n",
       " ('estat', -0.49066165085101465),\n",
       " ('. father', -0.49062311589259),\n",
       " ('program', -0.4905730586894684),\n",
       " ('( )', -0.4898181829460331),\n",
       " ('apart', -0.4896392506735755),\n",
       " ('text messag', -0.48929726978669236),\n",
       " ('] (', -0.48592930359559183),\n",
       " ('resid', -0.4855149981921365),\n",
       " ('video', -0.48482998135307054),\n",
       " ('told wa', -0.48244191841381356),\n",
       " ('contractor', -0.48198040583014357),\n",
       " ('cover', -0.4775470846095432),\n",
       " ('agreement', -0.47668500109818207),\n",
       " ('someth like', -0.47554919690412484),\n",
       " ('car', -0.4739175218065432),\n",
       " ('anyth ?', -0.47367735209632245),\n",
       " ('origin', -0.4732758059071741),\n",
       " ('ani legal', -0.46471420241585426),\n",
       " ('wa upset', -0.4645836091419111),\n",
       " ('. obvious', -0.4642961758056226),\n",
       " ('park', -0.4640922447546805),\n",
       " ('leg', -0.46099621841709937),\n",
       " ('. today', -0.46030014956239235),\n",
       " (\"'ve told\", -0.45985546216576845),\n",
       " ('fenc', -0.45946954858473316),\n",
       " ('appeal', -0.45616092418582205),\n",
       " (', friend', -0.45609127846398556),\n",
       " ('item', -0.4559800496382729),\n",
       " ('look like', -0.45287593484353883),\n",
       " ('read', -0.44970148177920743),\n",
       " ('wa ask', -0.4483515507947072),\n",
       " ('odd', -0.4478685971277975),\n",
       " ('risk', -0.4464521598018793),\n",
       " ('pictur', -0.446347833581551),\n",
       " ('child', -0.4460500889049292),\n",
       " ('fuck .', -0.44284059002459275),\n",
       " ('possibl', -0.44200884136706325),\n",
       " ('door', -0.4415444110924759),\n",
       " ('father', -0.441380842303723),\n",
       " ('believ', -0.44105387867113394),\n",
       " ('aunt', -0.44103874940815807),\n",
       " ('contact', -0.44062835840018866),\n",
       " ('took', -0.4402203835296365),\n",
       " ('thi just', -0.43972639253386775),\n",
       " ('want thi', -0.4390480690177199),\n",
       " ('answer', -0.43869391792336576),\n",
       " ('low', -0.4382380576161619),\n",
       " (\"'' ,\", -0.43674750317676886),\n",
       " ('public', -0.4352955884733468),\n",
       " ('note', -0.4339318807997072),\n",
       " ('feel realli', -0.4330427802750325),\n",
       " ('forc', -0.4310989565039182),\n",
       " ('notifi', -0.43098125576621077),\n",
       " ('student', -0.4295008943876515),\n",
       " ('compens', -0.42793832628838124),\n",
       " ('firm', -0.42681154341156036),\n",
       " ('togeth ,', -0.42633011764551193),\n",
       " ('california .', -0.4250428965751451),\n",
       " ('hr', -0.4247708957700902),\n",
       " ('40', -0.4232676506172731),\n",
       " ('determin', -0.4218592041472275),\n",
       " ('arriv', -0.4200464572831173),\n",
       " (', thank', -0.4191761561370873),\n",
       " ('wa just', -0.41915805067257195),\n",
       " ('fell', -0.4190659347141831),\n",
       " ('usa', -0.41859734756242634),\n",
       " ('thi year', -0.41825646117514403),\n",
       " ('. person', -0.4182215592911325),\n",
       " ('specif', -0.4175862446074539),\n",
       " ('design', -0.4174772434442793),\n",
       " ('light', -0.4166262795905607),\n",
       " ('track', -0.4152128407614874),\n",
       " ('thank', -0.4148840827045931),\n",
       " ('sober', -0.41484465836059037),\n",
       " ('collect', -0.41408403048076303),\n",
       " ('weed', -0.4138198249288162),\n",
       " ('ca', -0.41250242227993844),\n",
       " ('sent', -0.4120096169417884),\n",
       " ('stay .', -0.4113460641413495),\n",
       " ('. someth', -0.4111300881720494),\n",
       " ('doe', -0.4106138975582092),\n",
       " ('page', -0.4102549384574405),\n",
       " ('kick', -0.40949350323841904),\n",
       " ('correct', -0.4086875941120808),\n",
       " (\"'' (\", -0.4084519252053451),\n",
       " ('. night', -0.408256866971909),\n",
       " ('turn', -0.4075675651683339),\n",
       " ('paycheck', -0.4070111381923228),\n",
       " ('safeti', -0.4069293045632951),\n",
       " ('yard', -0.40665986672207155),\n",
       " ('danc', -0.4061637327199086),\n",
       " ('\\\\', -0.4059251222112975),\n",
       " ('t want', -0.4056252626933274),\n",
       " ('result', -0.40508386084352893),\n",
       " ('bond', -0.403029892181577),\n",
       " ('everyon ,', -0.40205575752399086),\n",
       " ('minut', -0.39975872520566674),\n",
       " ('happen', -0.39901742799561657),\n",
       " ('renew', -0.39815111794104724),\n",
       " ('owe', -0.3969305100840185),\n",
       " ('doctor', -0.39662909574432337),\n",
       " ('alot', -0.3963224030052109),\n",
       " (\"'d\", -0.3962656246515908),\n",
       " ('bank account', -0.39325645450542296),\n",
       " (\"friend 's\", -0.3932089975705761),\n",
       " ('make sens', -0.3925988772799446),\n",
       " ('famili .', -0.39109605827782495),\n",
       " ('group', -0.38990234789378037),\n",
       " ('cancel', -0.3893687728807009),\n",
       " ('. way', -0.38898919709676893),\n",
       " ('shop', -0.386093706475237),\n",
       " ('unabl', -0.38397150922936585),\n",
       " (\"say 'm\", -0.38379919652509403),\n",
       " ('book', -0.38337039600450457),\n",
       " ('knock', -0.3828577917670804),\n",
       " ('sever', -0.3828493538286189),\n",
       " ('flat', -0.38190274491869614),\n",
       " ('send', -0.3817570981303488),\n",
       " ('civil', -0.38135241761293137),\n",
       " ('. mom', -0.38113315848580503),\n",
       " ('thi guy', -0.38064516813841726),\n",
       " ('annoy', -0.38060716890377266),\n",
       " ('young', -0.3797489949816118),\n",
       " ('entitl', -0.3795047817031323),\n",
       " ('. ani', -0.37938017420229614),\n",
       " ('occur', -0.3787171119286907),\n",
       " ('destroy', -0.37856420238071126),\n",
       " ('treatment', -0.3779649392538733),\n",
       " ('street', -0.37758363144388474),\n",
       " ('pic', -0.3767251402226276),\n",
       " ('trash', -0.3759198561648559),\n",
       " (', howev', -0.37559930195682023),\n",
       " ('. *', -0.3746095955988951),\n",
       " ('. thank', -0.37430072581719404),\n",
       " ('invest', -0.3717196858679992),\n",
       " ('eye', -0.37089969689100866),\n",
       " ('( ', -0.3695036180560028),\n",
       " ('hous', -0.3693707688121105),\n",
       " (\"'m tri\", -0.3685120363121288),\n",
       " ('advanc .', -0.36735747910761035),\n",
       " ('-', -0.36652817345641064),\n",
       " ('releas', -0.36516419825391616),\n",
       " ('just need', -0.3638750361129779),\n",
       " ('base', -0.3629814835060498),\n",
       " ('parent .', -0.3617810487343534),\n",
       " ('carpet', -0.36176189012895393),\n",
       " (\"'' ``\", -0.3615800748477375),\n",
       " ('sit', -0.361199227322344),\n",
       " ('hour', -0.36077254902896777),\n",
       " ('applic', -0.36071041575134055),\n",
       " ('afterward', -0.3589875410750117),\n",
       " ('face', -0.35883146362095075),\n",
       " ('. point', -0.35640281065068724),\n",
       " ('build', -0.35620180349988584),\n",
       " ('lot .', -0.35538643132698305),\n",
       " ('wonder', -0.3548869537400083),\n",
       " ('4 month', -0.3544635798428836),\n",
       " ('chang', -0.35414347586188627),\n",
       " ('day .', -0.35342032937398565),\n",
       " ('mechan', -0.35285641054199335),\n",
       " ('okay', -0.35252337672366124),\n",
       " ('bc', -0.35234489188099544),\n",
       " ('wa veri', -0.35207412865706206),\n",
       " ('sexual', -0.35192838499492296),\n",
       " ('total', -0.3518423384903277),\n",
       " ('thi thing', -0.3517754021236957),\n",
       " ('probat', -0.35142221074500357),\n",
       " ('assist', -0.35083238879622713),\n",
       " ('stori short', -0.3502764459270106),\n",
       " ('begin', -0.3498654496038466),\n",
       " ('%', -0.3497875318311459),\n",
       " ('suicid', -0.3494253060384095),\n",
       " ('prepar', -0.34854724980621166),\n",
       " ('pet', -0.3478291388218551),\n",
       " ('search', -0.34590031604351773),\n",
       " ('consid', -0.3450569109166906),\n",
       " ('obvious', -0.34487606071805166),\n",
       " ('thi work', -0.3445952667262008),\n",
       " ('. guess', -0.3444956111179829),\n",
       " ('parti', -0.34439652917431274),\n",
       " ('bitch', -0.3436553123451454),\n",
       " ('number', -0.3429905023640609),\n",
       " ('new year', -0.3429097562071928),\n",
       " ('. thi', -0.3428185451266911),\n",
       " ('. girl', -0.34240660059397826),\n",
       " ('make sure', -0.3423827241870105),\n",
       " (\"'d like\", -0.3410379399218695),\n",
       " ('laugh', -0.3401797225900939),\n",
       " ('custom', -0.3401737343212641),\n",
       " ('mental', -0.33982883216186227),\n",
       " ('wa onli', -0.33977990207134207),\n",
       " ('child support', -0.33950206152897144),\n",
       " ('organ', -0.33926998437480754),\n",
       " ('good ,', -0.33786868065382475),\n",
       " ('previou', -0.3372436503577324),\n",
       " ('bathroom', -0.3368299450760966),\n",
       " ('shift', -0.33624054845070134),\n",
       " ('. understand', -0.33612900599507245),\n",
       " ('differ', -0.335734600463835),\n",
       " ('. thing', -0.3355179282663961),\n",
       " ('btw', -0.3343791973128278),\n",
       " ('agre', -0.3337385214012672),\n",
       " ('mobil', -0.33245083587212),\n",
       " ('. leav', -0.3321566898472456),\n",
       " ('propos', -0.33137383173970075),\n",
       " ('smoke', -0.3311928696858),\n",
       " ('refus', -0.3309414988461427),\n",
       " ('assault', -0.3309382181384262),\n",
       " ('provid', -0.3289189502389074),\n",
       " ('thi make', -0.3280732187903131),\n",
       " ('regard', -0.3280121294669256),\n",
       " ('4 year', -0.32773673612689597),\n",
       " ('legal ?', -0.3270207518118238),\n",
       " ('boyfriend ,', -0.326826070056408),\n",
       " ('follow', -0.3266553928771972),\n",
       " ('text ,', -0.32646156607218013),\n",
       " ('restaur', -0.3264423807682034),\n",
       " ('anyth like', -0.3263758322043148),\n",
       " ('mother', -0.3263187405238471),\n",
       " ('client', -0.3262613229021261),\n",
       " ('contribut', -0.32625908231344825),\n",
       " ('advanc', -0.32603486960373235),\n",
       " (\"n't think\", -0.3256011494124535),\n",
       " ('say ``', -0.32516616135167076),\n",
       " ('half', -0.3248631409443909),\n",
       " ('just realli', -0.3212896775799242),\n",
       " ('lock', -0.32034379079719205),\n",
       " ('difficult', -0.3199495609717703),\n",
       " ('. contact', -0.3196230168998248),\n",
       " ('respons .', -0.3193171346856567),\n",
       " ('14', -0.3188471406894853),\n",
       " (\"n't pay\", -0.31799735526119177),\n",
       " ('apolog', -0.3174684945385096),\n",
       " ('. girlfriend', -0.3168428555586666),\n",
       " ('type', -0.3167359883956444),\n",
       " ('10 year', -0.3157477959624945),\n",
       " ('post thi', -0.3149403627718492),\n",
       " ('free', -0.31422593056191106),\n",
       " ('. legal', -0.314032481043679),\n",
       " ('product', -0.31295152819349487),\n",
       " ('middl', -0.31233985876902326),\n",
       " ('intend', -0.3115634817363694),\n",
       " ('clear', -0.30995573370208207),\n",
       " ('remov', -0.30931327056974156),\n",
       " ('wake', -0.30893257466748236),\n",
       " ('babi', -0.30760972429043426),\n",
       " ('break .', -0.30662345536309754),\n",
       " ('anoth', -0.30640466437934877),\n",
       " ('return', -0.3062626901375675),\n",
       " ('feel .', -0.30620211117934937),\n",
       " ('poor', -0.3058898883168446),\n",
       " ('money', -0.3058864825059124),\n",
       " ('screenshot', -0.30480484112234063),\n",
       " ('pop', -0.3048044214208175),\n",
       " ('relat', -0.3042211274387227),\n",
       " ('finish', -0.3041911749799381),\n",
       " ('doesn ', -0.30387587123560306),\n",
       " ('doesn', -0.303642110325995),\n",
       " ('start feel', -0.3031682253021617),\n",
       " ('duti', -0.30316800421935886),\n",
       " ('exist', -0.30216403452787777),\n",
       " (', mani', -0.3020118841776197),\n",
       " ('edit', -0.30190116515851967),\n",
       " ('fee', -0.30136306210431835),\n",
       " ('warrant', -0.3005641343080004),\n",
       " ('furnitur', -0.30053737396512675),\n",
       " ('surgeri', -0.29956682773865795),\n",
       " ('recommend', -0.2988822528823698),\n",
       " ('. mani', -0.29847953440499075),\n",
       " ('normal .', -0.29838202755996407),\n",
       " ('thi someth', -0.29745851439140114),\n",
       " ('. explain', -0.2966053010018205),\n",
       " ('3 week', -0.2956223090425238),\n",
       " ('realli love', -0.29457979595454087),\n",
       " ('agent', -0.2942049678254556),\n",
       " ('pull', -0.29322859027561887),\n",
       " ('past year', -0.29313894529611395),\n",
       " ('exclus', -0.2927689174361335),\n",
       " ('die', -0.2926527210568551),\n",
       " (\". 've\", -0.2921251271625549),\n",
       " ('oh', -0.2920359343557405),\n",
       " ('. doe', -0.2919620741770066),\n",
       " ('( like', -0.2916604841339585),\n",
       " ('deduct', -0.29158751519844006),\n",
       " ('leav alon', -0.29080886853635524),\n",
       " ('schedul', -0.2902563873701153),\n",
       " ('card', -0.2900186255347727),\n",
       " ('land', -0.2890466062405398),\n",
       " ('involv', -0.28879234812774257),\n",
       " ('doe anyon', -0.2861476407422977),\n",
       " ('check', -0.2856012399251142),\n",
       " ('depart', -0.2850225480735861),\n",
       " ('worri', -0.2850161177759605),\n",
       " ('22', -0.2845441309280568),\n",
       " ('secur deposit', -0.28395830460383015),\n",
       " ('approv', -0.28387413564447633),\n",
       " ('teacher', -0.2836896392024966),\n",
       " ('hear .', -0.28322858046485544),\n",
       " ('boyfriend .', -0.2830069269971109),\n",
       " ('emerg', -0.2829170949651439),\n",
       " ('subject', -0.2823808295788597),\n",
       " ('incom', -0.2820340917545795),\n",
       " ('famili', -0.2809592429908403),\n",
       " ('sure .', -0.2809101408305233),\n",
       " ('sort', -0.28012760801475584),\n",
       " ('&', -0.2792123276627221),\n",
       " ('bunch', -0.27864960073648914),\n",
       " ('ha .', -0.2785457934854954),\n",
       " (\"n't ,\", -0.2782166700155663),\n",
       " ('death', -0.27709359149487206),\n",
       " ('instanc', -0.27701724829206886),\n",
       " ('amaz', -0.275937530665914),\n",
       " ('marri', -0.27547454666438215),\n",
       " ('attack', -0.27478631980416995),\n",
       " ('condit', -0.2745105013938074),\n",
       " (\"`` n't\", -0.2740747050019514),\n",
       " ('team', -0.2737622582534171),\n",
       " ('wa pretti', -0.2737458915896482),\n",
       " ('crime', -0.2736676261970528),\n",
       " ('anyth els', -0.27351940694745935),\n",
       " (\"n't believ\", -0.27228026229878716),\n",
       " ('disabl', -0.27198294356432023),\n",
       " ('languag', -0.2716807218389922),\n",
       " ('everyth wa', -0.27070307325644377),\n",
       " ('technic', -0.2702550671547655),\n",
       " ('fuck', -0.26971130253413955),\n",
       " ('summer', -0.2696688908475516),\n",
       " ('thank ani', -0.2689765039667394),\n",
       " ('appreci !', -0.2678925067988541),\n",
       " ('wa', -0.267475116609796),\n",
       " ('. talk', -0.26712709948938024),\n",
       " ('way ,', -0.26628201492254583),\n",
       " ('thu', -0.26608424047876306),\n",
       " ('said', -0.2659684274771921),\n",
       " ('oblig', -0.26586271207563034),\n",
       " ('small', -0.26530840380699905),\n",
       " ('becaus did', -0.2647576565817878),\n",
       " ('. ?', -0.26462234512468774),\n",
       " ('? edit', -0.26415518706371816),\n",
       " ('dog', -0.2638073388682506),\n",
       " ('. eventu', -0.26354324720613154),\n",
       " ('opportun', -0.2635264612255907),\n",
       " ('1st', -0.2620534408506934),\n",
       " ('thi week', -0.2617934508657963),\n",
       " ('room ,', -0.26160474553308927),\n",
       " (\". 're\", -0.2614394350531654),\n",
       " ('disgust', -0.2605505931448591),\n",
       " ('issu', -0.26051936237818996),\n",
       " ('account', -0.26006650586138397),\n",
       " ('googl', -0.25992592987661556),\n",
       " ('concern', -0.2599058702968451),\n",
       " ('17', -0.25977706143984775),\n",
       " (') thi', -0.25958910525465223),\n",
       " ('proof', -0.25921395437698064),\n",
       " ('. alreadi', -0.25902743609175355),\n",
       " ('pretti sure', -0.2589642445893903),\n",
       " ('bu', -0.2578578977584979),\n",
       " ('media', -0.2561409020571127),\n",
       " ('happen ,', -0.2557976947467585),\n",
       " ('els ,', -0.25574521167480463),\n",
       " ('tie', -0.25417555472907744),\n",
       " ('ye', -0.2541014514461208),\n",
       " ('insid', -0.25393790355594564),\n",
       " ('thank advanc', -0.25391705021540456),\n",
       " ('text say', -0.25391472083717204),\n",
       " ('mind ,', -0.2535036701954295),\n",
       " ('quick', -0.2531931495762202),\n",
       " ('supervisor', -0.25295818363613587),\n",
       " ('rate', -0.2524964231768992),\n",
       " (\". 's\", -0.2518462350759904),\n",
       " ('disput', -0.2517700629448887),\n",
       " ('t ani', -0.25135328428018344),\n",
       " ('vehicl', -0.2510483284897075),\n",
       " ('ton', -0.25066267821976335),\n",
       " ('. past', -0.25052086581576),\n",
       " ('thi .', -0.25025659176610526),\n",
       " ('scare', -0.2500852479856113),\n",
       " ('declin', -0.2497864757867284),\n",
       " ('hide', -0.249701240869583),\n",
       " ('option .', -0.24947425260567452),\n",
       " ('januari', -0.2486790204119299),\n",
       " ('. doesn', -0.24854764530269874),\n",
       " ('injuri', -0.24838318981630436),\n",
       " ('thi point', -0.24808163676118258),\n",
       " (\"( 'm\", -0.24791245728866262),\n",
       " (') .', -0.24784183959566403),\n",
       " ('. need', -0.24735485563873905),\n",
       " ('state ,', -0.2471634741286875),\n",
       " ('incid', -0.24713003928793936),\n",
       " ('blue', -0.24688227937209462),\n",
       " ('today', -0.24667704810947863),\n",
       " ('increas', -0.24623005987237886),\n",
       " ('becaus hi', -0.2461716717681363),\n",
       " ('wall', -0.2461566439090814),\n",
       " ('affect', -0.2457408543591939),\n",
       " ('past month', -0.24572140847855475),\n",
       " ('proper', -0.24565714087562607),\n",
       " ('did', -0.24556637472754056),\n",
       " (') ,', -0.2439182111719125),\n",
       " ('sure', -0.24388741239117617),\n",
       " ('told ', -0.24383310458610855),\n",
       " ('. follow', -0.2432871086276787),\n",
       " ('pay', -0.24318990712239474),\n",
       " ('deposit .', -0.2428585888093494),\n",
       " ('shortli', -0.24246478798327095),\n",
       " ('everyon', -0.24215945278524553),\n",
       " ('hello', -0.2413862287751902),\n",
       " ('weekend .', -0.24126601515197232),\n",
       " ('. feel', -0.24056325935334072),\n",
       " ('scream', -0.2398268858977904),\n",
       " (') ha', -0.23921567054825937),\n",
       " ('crimin', -0.2390494192620388),\n",
       " ('simpl', -0.23861578574387454),\n",
       " ('rape', -0.23851953373594353),\n",
       " ('know wa', -0.23839972055658232),\n",
       " ('thing .', -0.23829697701123775),\n",
       " ('facebook', -0.23823933038238898),\n",
       " ('consent', -0.23713114389176926),\n",
       " ('multipl', -0.2346620383742373),\n",
       " ('. mayb', -0.2341235855471829),\n",
       " ('lane', -0.23403972295538808),\n",
       " ('fine .', -0.2339839515910342),\n",
       " ('complic', -0.23316459623901717),\n",
       " ('success', -0.2331583270792685),\n",
       " ('everi night', -0.23313650110528192),\n",
       " ('. broke', -0.2326732728552542),\n",
       " ('dress', -0.2325910903895131),\n",
       " ('mean ,', -0.23108143482579319),\n",
       " (\"n't know\", -0.2298605150952111),\n",
       " ('recent', -0.2297602473589266),\n",
       " ('hi ,', -0.2297513941743393),\n",
       " ('need', -0.22954091861344145),\n",
       " ('nurs', -0.22920574799786825),\n",
       " ('post .', -0.2288204702492101),\n",
       " ('refund', -0.22865535062596515),\n",
       " ('t like', -0.2285593005126179),\n",
       " ('content', -0.22825390155121994),\n",
       " ('area', -0.22823320518794904),\n",
       " ('project', -0.22804412350312017),\n",
       " ('miser', -0.2280092655768429),\n",
       " ('uncomfort .', -0.2275835214905874),\n",
       " ('children', -0.22696931274593463),\n",
       " ('wa told', -0.22674836652767938),\n",
       " ('resolv', -0.22605648037993192),\n",
       " ('verbal', -0.22597434994521226),\n",
       " (', year', -0.22523032888512806),\n",
       " ('period', -0.22518901337206948),\n",
       " ('written', -0.22514133134247014),\n",
       " ('instead', -0.2249569370180685),\n",
       " ('hospit', -0.22491742073026288),\n",
       " ('busi', -0.22488288342790266),\n",
       " (', onli', -0.22472130514695754),\n",
       " ('. basic', -0.22463950231006613),\n",
       " ('stranger', -0.22420477238862432),\n",
       " ('morn .', -0.2241757768509109),\n",
       " ('smell', -0.22385189093151353),\n",
       " ('lost .', -0.2230377262820201),\n",
       " ('entir', -0.22254283906739783),\n",
       " ('( ,', -0.2219251325757479),\n",
       " ('sleep .', -0.22180343698870691),\n",
       " ('complaint', -0.2217301121926915),\n",
       " ('sorri', -0.2217222063255247),\n",
       " ('. veri', -0.221074284495273),\n",
       " ('care', -0.22089477097301666),\n",
       " ('. good', -0.220783793062887),\n",
       " ('aggress', -0.22000459559760246),\n",
       " (', want', -0.21909025942691318),\n",
       " ('address', -0.218836334408219),\n",
       " ('wa alway', -0.21847667264120213),\n",
       " ('mix', -0.21844713250766237),\n",
       " ('tell ', -0.2183399810502425),\n",
       " ('t .', -0.21833346452385127),\n",
       " ('coupl day', -0.21689223614732075),\n",
       " ('alcohol', -0.21680411241312106),\n",
       " ('colleg', -0.2155436261871099),\n",
       " ('snapchat', -0.21443004213434774),\n",
       " ('suit', -0.21427851644616588),\n",
       " ('black', -0.21405535520897218),\n",
       " ('ill', -0.21393757598799595),\n",
       " ('veri good', -0.21282296240291376),\n",
       " ('4', -0.21252282519837767),\n",
       " ('onc ,', -0.21247814743829146),\n",
       " ('abov', -0.21162423552628273),\n",
       " ('insight', -0.2112716537879418),\n",
       " (\", 'm\", -0.2106947663329908),\n",
       " ('link', -0.21045886354580945),\n",
       " ('valid', -0.21019759084819398),\n",
       " ('valu', -0.21012274120443344),\n",
       " ('fight .', -0.21008231325971216),\n",
       " ('court date', -0.209897702399887),\n",
       " ('offer', -0.20971496767657063),\n",
       " ('stop', -0.20920808034453808),\n",
       " ('1', -0.20913169799330428),\n",
       " ('assur', -0.20894567553072818),\n",
       " (\", 're\", -0.20873886447512532),\n",
       " ('octob', -0.20863490446515745),\n",
       " ('t anyth', -0.2084367277500606),\n",
       " ('id', -0.2084093956003068),\n",
       " ('john', -0.2083739495174643),\n",
       " ('board', -0.20828128862917472),\n",
       " (', anyth', -0.20827147891941566),\n",
       " ('speak', -0.20799352069069754),\n",
       " ('. liter', -0.2067943079499298),\n",
       " (\"becaus n't\", -0.20654624840283808),\n",
       " ('internet', -0.20646078975536622),\n",
       " ('let .', -0.20628639550158015),\n",
       " ('piec', -0.20504130364234935),\n",
       " ('doe thi', -0.20488666593655258),\n",
       " ('love .', -0.2046172607433409),\n",
       " ('peopl', -0.20455997524962644),\n",
       " ('loan', -0.20433755519008726),\n",
       " ('heat', -0.20367316463275734),\n",
       " (', gave', -0.20318560433812768),\n",
       " ('car wa', -0.20285612548894852),\n",
       " ('30 day', -0.2024351222256586),\n",
       " ('mutual', -0.2019389016755158),\n",
       " ('relev', -0.20153100285398695),\n",
       " ('email', -0.20133413642597994),\n",
       " ('fals', -0.2012236305592418),\n",
       " (', month', -0.2010505653055048),\n",
       " ('left .', -0.20102428655338084),\n",
       " ('expir', -0.2008658051957361),\n",
       " ('. actual', -0.20044921610189506),\n",
       " ('think ,', -0.19954544947442082),\n",
       " ('obviou', -0.19905669605005075),\n",
       " ('quiet', -0.1989472239937016),\n",
       " ('known', -0.19825300548545194),\n",
       " ('later ,', -0.1973647352088114),\n",
       " ('stand', -0.1973346611459013),\n",
       " ('mom ,', -0.1973081966093238),\n",
       " ('punch', -0.19694835280111098),\n",
       " ('march', -0.19690401089183718),\n",
       " ('plan ,', -0.1968658731206545),\n",
       " ('oper', -0.19682820724186254),\n",
       " ('eventu', -0.19626890542082373),\n",
       " ('year ago', -0.19588464267249767),\n",
       " (\"'ve talk\", -0.19576315116047774),\n",
       " ('exactli', -0.19528259572713855),\n",
       " (\"? 's\", -0.1949060199148764),\n",
       " ('great', -0.19418651295442463),\n",
       " ('cours', -0.1938258582238078),\n",
       " ('just leav', -0.19381276279666032),\n",
       " ('tax', -0.19357735167058676),\n",
       " ('support .', -0.1935360977951386),\n",
       " ('mom', -0.19352697289668805),\n",
       " ('thank !', -0.19316711272863746),\n",
       " ('wa tri', -0.19300897686634189),\n",
       " ('believ .', -0.19287228326943484),\n",
       " ('true .', -0.19226289626096643),\n",
       " ('sure ,', -0.19226069075205035),\n",
       " ('hand', -0.19195329004658732),\n",
       " ('dont know', -0.19187737976986644),\n",
       " ('befor wa', -0.1910235402909972),\n",
       " ('perman', -0.19080007521683018),\n",
       " ('need ,', -0.1907921086846647),\n",
       " ('develop', -0.1906956977971765),\n",
       " (\". 'm\", -0.19067833585181934),\n",
       " ('monday', -0.19053740176371284),\n",
       " ('dad', -0.19012872696524497),\n",
       " ('? (', -0.18990490989795714),\n",
       " (', felt', -0.1895694393883915),\n",
       " ('plu', -0.18947707438158307),\n",
       " (\"'s ,\", -0.18920100254068567),\n",
       " ('agenc', -0.18886814005194535),\n",
       " ('consult', -0.1886888638148198),\n",
       " ('especi', -0.18851225292223733),\n",
       " ('plate', -0.18848468225379636),\n",
       " ('t talk', -0.18811369473493775),\n",
       " ('messag', -0.18671066898083735),\n",
       " ('clearli', -0.186691959909339),\n",
       " ('woke', -0.18662638445030755),\n",
       " ('dealership', -0.18661795196783995),\n",
       " ('bad .', -0.186437668400561),\n",
       " ('time togeth', -0.18623019775544408),\n",
       " ('hurt ,', -0.18591308878578244),\n",
       " ('usual', -0.1853224839252969),\n",
       " ('. guy', -0.1846539774217799),\n",
       " ('avoid', -0.1842594399644042),\n",
       " ('ladi', -0.1842373524287445),\n",
       " ('flip', -0.18387378755022293),\n",
       " (', parent', -0.1838064698043042),\n",
       " ('23', -0.18322661985889313),\n",
       " (', doe', -0.18311279340327521),\n",
       " ('broken', -0.18297189413004117),\n",
       " ('stori ,', -0.18267739118281917),\n",
       " ('exampl ,', -0.18241623051078518),\n",
       " ('month befor', -0.1820936081979484),\n",
       " ('hole', -0.18178609018868383),\n",
       " (', ex', -0.18144330538119843),\n",
       " ('. felt', -0.18140384498232223),\n",
       " ('career', -0.18064691440143527),\n",
       " ('. dad', -0.18013895883055858),\n",
       " ('becaus thi', -0.18002416768493876),\n",
       " ('miss .', -0.18001995864578943),\n",
       " ('reach', -0.1800096550833692),\n",
       " ('phone ,', -0.17988467158231025),\n",
       " ('list', -0.179775743319603),\n",
       " ('behavior', -0.17956989204057022),\n",
       " ('doesnt', -0.17949056202518543),\n",
       " ('discov', -0.17903483486321753),\n",
       " ('lawsuit', -0.17886559575874994),\n",
       " ('respons', -0.17861065373980448),\n",
       " ('time thi', -0.17835788239215172),\n",
       " ('abandon', -0.1779936868054089),\n",
       " ('? ani', -0.17794986637895194),\n",
       " ('unit', -0.1778276207343905),\n",
       " ('word', -0.17781790373372577),\n",
       " ('thank read', -0.1777842371080245),\n",
       " (\"becaus 's\", -0.17743413778335965),\n",
       " ('ha', -0.1771958907587523),\n",
       " ('charg .', -0.17719337729566556),\n",
       " ('arm', -0.17677000476017923),\n",
       " ('long stori', -0.1763077990753905),\n",
       " (\"? n't\", -0.17614317698040835),\n",
       " ('wors', -0.17608741387449875),\n",
       " ('week .', -0.1758815054925744),\n",
       " ('sister', -0.17560674866525222),\n",
       " ('line', -0.17559648235953082),\n",
       " ('\\\\-', -0.17553856410634033),\n",
       " ('fix', -0.17551466478441635),\n",
       " ('fast forward', -0.1753642508256116),\n",
       " ('essenti', -0.1752782499233683),\n",
       " ('. date', -0.17518919341855613),\n",
       " ('curiou', -0.17497154082083352),\n",
       " (\"? 'm\", -0.17461549757899372),\n",
       " ('price', -0.17442307362053727),\n",
       " ('onli thing', -0.17427921445448427),\n",
       " ('els .', -0.17406967818032673),\n",
       " ('electr', -0.17392596979966288),\n",
       " ('. everi', -0.1738973978378588),\n",
       " ('rush', -0.17373182843705304),\n",
       " ('aren ', -0.17353525482449708),\n",
       " ('benefit', -0.1735050371182235),\n",
       " ('sinc ', -0.17336631345887032),\n",
       " ('work .', -0.1732736251708134),\n",
       " ('hit .', -0.1731250086562979),\n",
       " ('gay', -0.1729638857021986),\n",
       " ('possibl .', -0.17276192816201624),\n",
       " ('guy .', -0.17196597396503588),\n",
       " ('dont want', -0.17137258011555362),\n",
       " ('buddi', -0.1708072360370342),\n",
       " (\"'' .\", -0.17066691410153595),\n",
       " ('hotel', -0.1706321517614198),\n",
       " ('abl', -0.16917110755016976),\n",
       " ('regular', -0.16862415967664737),\n",
       " ('aren', -0.16801154819010955),\n",
       " ('thousand', -0.1679426419984621),\n",
       " ('themselv', -0.16771112269675503),\n",
       " ('. met', -0.1670967762390592),\n",
       " ('think .', -0.16692344228893616),\n",
       " ('june', -0.1661435517012914),\n",
       " ('yesterday', -0.1661197303382718),\n",
       " ('tabl', -0.16603154269321915),\n",
       " ('like want', -0.16583243525946403),\n",
       " (\"n't talk\", -0.16582183431921965),\n",
       " ('hour later', -0.16576646638316503),\n",
       " ('. took', -0.16421323872102336),\n",
       " ('ground', -0.16355100818984308),\n",
       " ('request', -0.16333678447009445),\n",
       " ('dollar', -0.16287293248353235),\n",
       " ('. 3', -0.16263153888022644),\n",
       " ('profession', -0.16216525965014766),\n",
       " (\"'s like\", -0.16212330502986663),\n",
       " ('photo', -0.16209786485131442),\n",
       " ('wrong .', -0.16204022273111152),\n",
       " ('carri', -0.1619814826485467),\n",
       " ('non', -0.16138252623518246),\n",
       " ('wa feel', -0.16132820142784005),\n",
       " ('health', -0.1613043652910027),\n",
       " ('date .', -0.1611708757048919),\n",
       " ('60', -0.1608520081799645),\n",
       " ('rent', -0.1606614430636452),\n",
       " ('mold', -0.1601333618471333),\n",
       " ('screw', -0.16009743566961676),\n",
       " ('want leav', -0.15968271014068763),\n",
       " ('came home', -0.1592565104205458),\n",
       " ('stuck', -0.15924497810056928),\n",
       " ('everi day', -0.15869782412990963),\n",
       " ('ago wa', -0.15854373811591796),\n",
       " ('thi ?', -0.1583268589327867),\n",
       " ('ago ,', -0.15798631430518686),\n",
       " ('reddit', -0.1576378075963664),\n",
       " ('place .', -0.15759520143107345),\n",
       " ('hard time', -0.15744249137871502),\n",
       " ('ga', -0.1573982073128128),\n",
       " ('broke .', -0.1573376497087759),\n",
       " ('wa ,', -0.15712064827981204),\n",
       " ('heard', -0.15706140674914876),\n",
       " ('term', -0.15694372396367906),\n",
       " ('prevent', -0.15692001544400525),\n",
       " ('thi', -0.15691897211624115),\n",
       " ('read .', -0.15681006043250023),\n",
       " ('& gt', -0.15634364721696675),\n",
       " ('gt ;', -0.15634364721696675),\n",
       " ('individu', -0.15619617303980837),\n",
       " (\"like 'm\", -0.1561932135073725),\n",
       " ('. landlord', -0.1561007859381239),\n",
       " (', date', -0.15607746952483834),\n",
       " ('statement', -0.1559105738599954),\n",
       " ('strang', -0.1558184623770822),\n",
       " ('2017', -0.15578261436958638),\n",
       " ('throwaway', -0.1557376501681207),\n",
       " ('feel veri', -0.155429031154856),\n",
       " ('inspect', -0.15402483040245257),\n",
       " ('becaus wa', -0.15399954015162595),\n",
       " (\"'m veri\", -0.15365227390600486),\n",
       " ('sure wa', -0.15341350949561344),\n",
       " ('m realli', -0.1531563722401416),\n",
       " ('music', -0.1529230532759855),\n",
       " ('wa thi', -0.1518222279329307),\n",
       " ('just let', -0.151798896152456),\n",
       " ('water', -0.15074582321356875),\n",
       " ('younger', -0.15051061278232314),\n",
       " ('form', -0.15025653914258355),\n",
       " ('mortgag', -0.1498910055294735),\n",
       " ('coupl', -0.1492954553465193),\n",
       " ('forth', -0.14863964028594848),\n",
       " ('pay .', -0.14862732150857297),\n",
       " ('sound', -0.14846079316478414),\n",
       " ('. edit', -0.14832988193503632),\n",
       " ('want friend', -0.14809405222174374),\n",
       " ('hear', -0.14804261739431618),\n",
       " ('got', -0.14769872972782777),\n",
       " ('marri ,', -0.1471883829225163),\n",
       " ('driveway', -0.14694245499620806),\n",
       " ('uncomfort', -0.14669822769277358),\n",
       " ('ani way', -0.14656201807728586),\n",
       " ('seriou', -0.14645596440570932),\n",
       " ('rememb', -0.1464492199483139),\n",
       " ('real', -0.1461097903782371),\n",
       " (', refus', -0.14570320170229636),\n",
       " ('thi legal', -0.14535390493227368),\n",
       " (\"'m worri\", -0.14483008507965925),\n",
       " ('. want', -0.14461635028785863),\n",
       " ('depend', -0.14454052001699644),\n",
       " ('control', -0.14437982559582496),\n",
       " (', care', -0.1441054467043165),\n",
       " ('just feel', -0.14383366195477812),\n",
       " ('. read', -0.1437300200773214),\n",
       " ('wo', -0.1435347978344116),\n",
       " (\"wo n't\", -0.1435347978344116),\n",
       " ('check .', -0.14351816617888638),\n",
       " (', got', -0.14335073871017417),\n",
       " ('sex ,', -0.14252069678507087),\n",
       " (', ?', -0.14237532233805228),\n",
       " ('evid', -0.14225947312730403),\n",
       " ('clue', -0.1420152010522127),\n",
       " ('truli', -0.14176145849012542),\n",
       " ('site', -0.1414212348520433),\n",
       " ('thi weekend', -0.14130566500024774),\n",
       " ('just make', -0.14057796484104002),\n",
       " (', alway', -0.14047645018733845),\n",
       " ('thi kind', -0.14040134930514436),\n",
       " ('im', -0.14035641755879827),\n",
       " (', come', -0.14007733290322522),\n",
       " ('counsel', -0.13984337678717),\n",
       " ('desper', -0.13939286187449695),\n",
       " ('parent', -0.13925805202643482),\n",
       " ('. haven', -0.13838094936466006),\n",
       " ('realli care', -0.138198759008914),\n",
       " ('tell thi', -0.1380298027652132),\n",
       " ('hour .', -0.13710104511452526),\n",
       " ('format', -0.13684938124988338),\n",
       " (', veri', -0.1363278124046542),\n",
       " ('morn', -0.13620501603048865),\n",
       " ('bed', -0.13530358466661718),\n",
       " ('post', -0.13466910291666917),\n",
       " ('traffic', -0.13461350045716938),\n",
       " ('sat', -0.13449127211492617),\n",
       " ('mainten', -0.13430911532560172),\n",
       " ('. leas', -0.13402572674578278),\n",
       " ('regardless', -0.13384140116610585),\n",
       " ('won', -0.13383969938162224),\n",
       " ('safe', -0.13366667656162906),\n",
       " ('isn ', -0.13312902473026902),\n",
       " ('sens', -0.13274620934477396),\n",
       " ('becaus love', -0.13251221742511482),\n",
       " ('. know', -0.13249271805251156),\n",
       " ('seller', -0.13239820188275353),\n",
       " ('isn', -0.13235378241405915),\n",
       " ('tri figur', -0.13225850371474834),\n",
       " ('thi time', -0.13215298530383218),\n",
       " ('month ,', -0.1316592644718134),\n",
       " (', befor', -0.1310741576430424),\n",
       " (', claim', -0.13099744411341524),\n",
       " ('lunch', -0.13078230034312954),\n",
       " (', lot', -0.13071699974430628),\n",
       " ('short ,', -0.13024366562485556),\n",
       " ('certain', -0.1301999780291509),\n",
       " ('. notic', -0.12997209789342687),\n",
       " ('told', -0.12996649646740022),\n",
       " ('colleg .', -0.12991058403069644),\n",
       " ('roommat', -0.12905493147929523),\n",
       " ('game', -0.12895561575580544),\n",
       " ('flight', -0.12813446974657974),\n",
       " ('5 year', -0.12781622749777455),\n",
       " ('fault .', -0.12721212162009202),\n",
       " ('becaus', -0.12705974947998025),\n",
       " ('honestli', -0.12695080333382255),\n",
       " ('work', -0.1261080062090925),\n",
       " ('polic report', -0.12581248148427115),\n",
       " ('. someon', -0.12503812961121114),\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:26:15.367908Z",
     "start_time": "2019-04-08T15:25:28.825147Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7910219675262655"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr.score(X_train_com, y_train_com)\n",
    "\n",
    "gs_lr.score(X_test_com, y_test_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:38:19.386869Z",
     "start_time": "2019-04-08T15:38:19.365126Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr3 = Pipeline([\n",
    "    ('cv', CountVectorizer(tokenizer=LemmaTokenizer())),\n",
    "    ('lr', LogisticRegression(n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cv__stop_words' : ['english'],\n",
    "    'cv__max_features': [3000],\n",
    "    'cv__min_df': [2],\n",
    "    'cv__max_df': [.9],\n",
    "    'cv__ngram_range': [(1,2)] #Tried (1,3) - (1,2) performed better\n",
    "#     'lr__solver' : ['newton-cg']\n",
    "#     'lr__penalty' : ['l2'],\n",
    "#     'lr__C': [.1, .5]\n",
    "}\n",
    "\n",
    "gs_lr3 = GridSearchCV(pipe_lr3, param_grid=pipe_params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:40:28.988965Z",
     "start_time": "2019-04-08T15:38:19.538286Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_a...penalty='l2', random_state=None, solver='warn', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'cv__stop_words': ['english'], 'cv__max_features': [3000], 'cv__min_df': [2], 'cv__max_df': [0.9], 'cv__ngram_range': [(1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr3.fit(X_train_com, y_train_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:40:29.035219Z",
     "start_time": "2019-04-08T15:40:28.993974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '! !',\n",
       " \"! ''\",\n",
       " '! ',\n",
       " '#',\n",
       " '# wiki_general_rules',\n",
       " '# x200b',\n",
       " '$',\n",
       " '$ $',\n",
       " '%',\n",
       " '% 2flegaladvice',\n",
       " '% 2fr',\n",
       " '&',\n",
       " '& amp',\n",
       " '& gt',\n",
       " \"'\",\n",
       " \"' .\",\n",
       " \"''\",\n",
       " \"'' 's\",\n",
       " \"'' ,\",\n",
       " \"'' .\",\n",
       " \"'' ?\",\n",
       " \"'' ``\",\n",
       " \"'d\",\n",
       " \"'d like\",\n",
       " \"'d say\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'m going\",\n",
       " \"'m just\",\n",
       " \"'m really\",\n",
       " \"'m saying\",\n",
       " \"'m sorry\",\n",
       " \"'m sure\",\n",
       " \"'m trying\",\n",
       " \"'re\",\n",
       " \"'re doing\",\n",
       " \"'re going\",\n",
       " \"'re just\",\n",
       " \"'re looking\",\n",
       " \"'re right\",\n",
       " \"'s\",\n",
       " \"'s ,\",\n",
       " \"'s .\",\n",
       " \"'s ``\",\n",
       " \"'s best\",\n",
       " \"'s doing\",\n",
       " \"'s going\",\n",
       " \"'s good\",\n",
       " \"'s hard\",\n",
       " \"'s just\",\n",
       " \"'s legal\",\n",
       " \"'s like\",\n",
       " \"'s likely\",\n",
       " \"'s okay\",\n",
       " \"'s possible\",\n",
       " \"'s probably\",\n",
       " \"'s problem\",\n",
       " \"'s really\",\n",
       " \"'s right\",\n",
       " \"'s thing\",\n",
       " \"'s time\",\n",
       " \"'s way\",\n",
       " \"'s worth\",\n",
       " \"'s wrong\",\n",
       " \"'ve\",\n",
       " \"'ve got\",\n",
       " \"'ve seen\",\n",
       " '(',\n",
       " '( )',\n",
       " '( ,',\n",
       " '( /message/compose/',\n",
       " '( coming',\n",
       " '( http',\n",
       " '( like',\n",
       " \"( n't\",\n",
       " '( s',\n",
       " '( wa',\n",
       " '( ',\n",
       " ')',\n",
       " ') &',\n",
       " \") 's\",\n",
       " ') )',\n",
       " ') *',\n",
       " ') ,',\n",
       " ') .',\n",
       " ') :',\n",
       " ') ^|',\n",
       " ') question',\n",
       " ') section',\n",
       " ') |',\n",
       " '*',\n",
       " '* *do',\n",
       " '* *i',\n",
       " '* --',\n",
       " '* [',\n",
       " '* post',\n",
       " '**',\n",
       " '** --',\n",
       " '** original',\n",
       " '***',\n",
       " '*** --',\n",
       " '***do',\n",
       " '***do delete',\n",
       " '**it',\n",
       " '**it appears',\n",
       " '*do',\n",
       " '*do reach',\n",
       " '*i',\n",
       " '*i bot',\n",
       " '*if',\n",
       " '*if feel',\n",
       " '*please',\n",
       " '*please [',\n",
       " '*your',\n",
       " '*your comment',\n",
       " '*your post',\n",
       " ',',\n",
       " ', &',\n",
       " \", ''\",\n",
       " \", 'd\",\n",
       " \", 'll\",\n",
       " \", 'm\",\n",
       " \", 're\",\n",
       " \", 's\",\n",
       " \", 've\",\n",
       " ', (',\n",
       " ', )',\n",
       " ', ,',\n",
       " ', .',\n",
       " ', ?',\n",
       " ', [',\n",
       " ', ``',\n",
       " ', able',\n",
       " ', absolutely',\n",
       " ', action',\n",
       " ', actually',\n",
       " ', ask',\n",
       " ', believe',\n",
       " ', best',\n",
       " ', better',\n",
       " ', ca',\n",
       " ', case',\n",
       " ', child',\n",
       " ', come',\n",
       " ', contact',\n",
       " ', definitely',\n",
       " ', did',\n",
       " ', disregard',\n",
       " ', doe',\n",
       " ', doesn',\n",
       " ', don',\n",
       " ', dont',\n",
       " ', edit',\n",
       " ', end',\n",
       " ', especially',\n",
       " ', fact',\n",
       " ', feel',\n",
       " ', friend',\n",
       " ', getting',\n",
       " ', going',\n",
       " ', good',\n",
       " ', got',\n",
       " ', guess',\n",
       " ', guy',\n",
       " ', ha',\n",
       " ', having',\n",
       " ', help',\n",
       " ', hope',\n",
       " ', including',\n",
       " ', just',\n",
       " ', kind',\n",
       " ', know',\n",
       " ', landlord',\n",
       " ', lawyer',\n",
       " ', leave',\n",
       " ', legal',\n",
       " ', let',\n",
       " ', life',\n",
       " ', like',\n",
       " ', likely',\n",
       " ', long',\n",
       " ', look',\n",
       " ', lot',\n",
       " ', love',\n",
       " ', make',\n",
       " ', matter',\n",
       " ', maybe',\n",
       " ', mean',\n",
       " ', message',\n",
       " ', money',\n",
       " \", n't\",\n",
       " ', need',\n",
       " ', new',\n",
       " ', op',\n",
       " ', parent',\n",
       " ', pay',\n",
       " ', people',\n",
       " ', person',\n",
       " ', police',\n",
       " ', probably',\n",
       " ', problem',\n",
       " ', really',\n",
       " ', reason',\n",
       " ', relationship',\n",
       " ', reply',\n",
       " ', right',\n",
       " ', said',\n",
       " ', say',\n",
       " ', simply',\n",
       " ', sound',\n",
       " ', start',\n",
       " ', stop',\n",
       " ', sure',\n",
       " ', talk',\n",
       " ', tell',\n",
       " ', thank',\n",
       " ', thanks',\n",
       " ', thing',\n",
       " ', think',\n",
       " ', thought',\n",
       " ', time',\n",
       " ', told',\n",
       " ', try',\n",
       " ', understand',\n",
       " ', unless',\n",
       " ', use',\n",
       " ', wa',\n",
       " ', want',\n",
       " ', way',\n",
       " ', went',\n",
       " ', wo',\n",
       " ', work',\n",
       " ', yes',\n",
       " ', ',\n",
       " ', ',\n",
       " '-',\n",
       " '- &',\n",
       " \"- 's\",\n",
       " '- ***do',\n",
       " '- **it',\n",
       " '- *i',\n",
       " '- [',\n",
       " '- author',\n",
       " '- instead',\n",
       " '- locationbot',\n",
       " '--',\n",
       " '-- -',\n",
       " '-- --',\n",
       " '.',\n",
       " '. &',\n",
       " \". ''\",\n",
       " \". 'd\",\n",
       " \". 'll\",\n",
       " \". 'm\",\n",
       " \". 're\",\n",
       " \". 's\",\n",
       " \". 've\",\n",
       " '. (',\n",
       " '. )',\n",
       " '. *',\n",
       " '. **',\n",
       " '. ***',\n",
       " '. *i',\n",
       " '. *if',\n",
       " '. *please',\n",
       " '. ,',\n",
       " '. --',\n",
       " '. .',\n",
       " '. ...',\n",
       " '. 2',\n",
       " '. 2.',\n",
       " '. 3',\n",
       " '. :',\n",
       " '. ?',\n",
       " '. [',\n",
       " '. ]',\n",
       " '. ``',\n",
       " '. able',\n",
       " '. actually',\n",
       " '. advice',\n",
       " '. agree',\n",
       " '. appears',\n",
       " '. ask',\n",
       " '. asked',\n",
       " '. basically',\n",
       " '. believe',\n",
       " '. best',\n",
       " '. better',\n",
       " '. boyfriend',\n",
       " '. break',\n",
       " '. ca',\n",
       " '. care',\n",
       " '. case',\n",
       " '. change',\n",
       " '. come',\n",
       " '. consider',\n",
       " '. contact',\n",
       " '. course',\n",
       " '. court',\n",
       " '. day',\n",
       " '. decide',\n",
       " '. definitely',\n",
       " '. did',\n",
       " '. didn',\n",
       " '. doe',\n",
       " '. doesn',\n",
       " '. doing',\n",
       " '. don',\n",
       " '. dont',\n",
       " '. edit',\n",
       " '. end',\n",
       " '. especially',\n",
       " '. eventually',\n",
       " '. ex',\n",
       " '. example',\n",
       " '. fact',\n",
       " '. family',\n",
       " '. far',\n",
       " '. father',\n",
       " '. feel',\n",
       " '. file',\n",
       " '. friend',\n",
       " '. fuck',\n",
       " '. generally',\n",
       " '. getting',\n",
       " '. girl',\n",
       " '. girlfriend',\n",
       " '. going',\n",
       " '. good',\n",
       " '. got',\n",
       " '. guess',\n",
       " '. guy',\n",
       " '. ha',\n",
       " '. happens',\n",
       " '. having',\n",
       " '. help',\n",
       " '. honestly',\n",
       " '. hope',\n",
       " '. hopefully',\n",
       " '. http',\n",
       " '. husband',\n",
       " '. idea',\n",
       " '. im',\n",
       " '. instead',\n",
       " '. insurance',\n",
       " '. just',\n",
       " '. kid',\n",
       " '. kind',\n",
       " '. knew',\n",
       " '. know',\n",
       " '. landlord',\n",
       " '. lawyer',\n",
       " '. leave',\n",
       " '. legal',\n",
       " '. let',\n",
       " '. life',\n",
       " '. like',\n",
       " '. likely',\n",
       " '. live',\n",
       " '. long',\n",
       " '. look',\n",
       " '. lot',\n",
       " '. love',\n",
       " '. make',\n",
       " '. matter',\n",
       " '. maybe',\n",
       " '. mean',\n",
       " '. mother',\n",
       " \". n't\",\n",
       " '. need',\n",
       " '. new',\n",
       " '. op',\n",
       " '. option',\n",
       " '. paid',\n",
       " '. parent',\n",
       " '. pay',\n",
       " '. people',\n",
       " '. person',\n",
       " '. personally',\n",
       " '. point',\n",
       " '. police',\n",
       " '. post',\n",
       " '. probably',\n",
       " '. problem',\n",
       " '. question',\n",
       " '. read',\n",
       " '. real',\n",
       " '. really',\n",
       " '. reason',\n",
       " '. relationship',\n",
       " '. remember',\n",
       " '. reply',\n",
       " '. right',\n",
       " '. said',\n",
       " '. say',\n",
       " '. saying',\n",
       " '. second',\n",
       " '. seriously',\n",
       " '. sex',\n",
       " '. situation',\n",
       " '. sorry',\n",
       " '. sound',\n",
       " '. start',\n",
       " '. state',\n",
       " '. stay',\n",
       " '. stop',\n",
       " '. sure',\n",
       " '. talk',\n",
       " '. tell',\n",
       " '. thank',\n",
       " '. thanks',\n",
       " '. thing',\n",
       " '. think',\n",
       " '. thought',\n",
       " '. time',\n",
       " '. told',\n",
       " '. took',\n",
       " '. tried',\n",
       " '. trust',\n",
       " '. try',\n",
       " '. trying',\n",
       " '. understand',\n",
       " '. unfortunately',\n",
       " '. unless',\n",
       " '. update',\n",
       " '. use',\n",
       " '. used',\n",
       " '. usually',\n",
       " '. wa',\n",
       " '. want',\n",
       " '. wanted',\n",
       " '. way',\n",
       " '. went',\n",
       " '. wife',\n",
       " '. wish',\n",
       " '. wo',\n",
       " '. woman',\n",
       " '. work',\n",
       " '. yeah',\n",
       " '. year',\n",
       " '. yes',\n",
       " '. ',\n",
       " '..',\n",
       " '...',\n",
       " \"... 's\",\n",
       " '... .',\n",
       " '/',\n",
       " '//imgur.com/a/myiab',\n",
       " '//imgur.com/a/myiab --',\n",
       " '//locationbot.info',\n",
       " '//locationbot.info )',\n",
       " '//www.reddit.com/message/compose',\n",
       " '//www.reddit.com/message/compose ?',\n",
       " '//www.reddit.com/r/legaladvice/wiki/index',\n",
       " '//www.reddit.com/r/legaladvice/wiki/index #',\n",
       " '//www.reddit.com/r/legaladvice/wiki/landlord',\n",
       " '//www.reddit.com/r/legaladvice/wiki/landlord )',\n",
       " '//www.reddit.com/r/locationbot',\n",
       " '//www.reddit.com/r/locationbot )',\n",
       " '/message/compose/',\n",
       " '/message/compose/ ?',\n",
       " '/r/legaladvice',\n",
       " '/r/legaladvice wiki',\n",
       " '1',\n",
       " '1 )',\n",
       " '1.',\n",
       " '10',\n",
       " '10 year',\n",
       " '100',\n",
       " '100 %',\n",
       " '12',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '18',\n",
       " '1st',\n",
       " '2',\n",
       " '2 )',\n",
       " '2 year',\n",
       " '2.',\n",
       " '20',\n",
       " '21',\n",
       " '2flegaladvice',\n",
       " '2flegaladvice )',\n",
       " '2fr',\n",
       " '2fr %',\n",
       " '3',\n",
       " '3 )',\n",
       " '3 year',\n",
       " '3.',\n",
       " '30',\n",
       " '30 day',\n",
       " '32',\n",
       " '3rd',\n",
       " '4',\n",
       " '4.0',\n",
       " '4.0 |',\n",
       " '4.31977192',\n",
       " '4.31977192 |',\n",
       " '40',\n",
       " '5',\n",
       " '5 year',\n",
       " '50',\n",
       " '500',\n",
       " '6',\n",
       " '6 month',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ': &',\n",
       " ': (',\n",
       " ': )',\n",
       " ': *',\n",
       " ': //imgur.com/a/myiab',\n",
       " ': //locationbot.info',\n",
       " ': //www.reddit.com/message/compose',\n",
       " ': //www.reddit.com/r/legaladvice/wiki/index',\n",
       " ': //www.reddit.com/r/legaladvice/wiki/landlord',\n",
       " ': //www.reddit.com/r/locationbot',\n",
       " ': http',\n",
       " ';',\n",
       " '; #',\n",
       " '; &',\n",
       " \"; 'm\",\n",
       " \"; 's\",\n",
       " '; (',\n",
       " '; ,',\n",
       " '; --',\n",
       " '; amp',\n",
       " '; http',\n",
       " '; just',\n",
       " \"; n't\",\n",
       " '; nbsp',\n",
       " '; wa',\n",
       " '; ',\n",
       " '?',\n",
       " '? !',\n",
       " '? &',\n",
       " \"? ''\",\n",
       " \"? 'm\",\n",
       " \"? 're\",\n",
       " \"? 's\",\n",
       " '? (',\n",
       " '? )',\n",
       " '? **',\n",
       " '? ,',\n",
       " '? --',\n",
       " '? .',\n",
       " '? ?',\n",
       " '? did',\n",
       " '? doe',\n",
       " '? ha',\n",
       " '? just',\n",
       " '? know',\n",
       " '? like',\n",
       " '? maybe',\n",
       " \"? n't\",\n",
       " '? sound',\n",
       " '? think',\n",
       " '? to=',\n",
       " '? to=/r/legaladvice',\n",
       " '? wa',\n",
       " '? want',\n",
       " '? yes',\n",
       " '? ',\n",
       " '[',\n",
       " '[ commenting',\n",
       " '[ contact',\n",
       " '[ landlord',\n",
       " '[ message',\n",
       " '[ moderator',\n",
       " '[ read',\n",
       " '[ report',\n",
       " '[ statistic',\n",
       " ']',\n",
       " '] (',\n",
       " '^',\n",
       " '^ [',\n",
       " '^|',\n",
       " '``',\n",
       " \"`` ''\",\n",
       " '`` hey',\n",
       " \"`` n't\",\n",
       " 'ability',\n",
       " 'able',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abuse',\n",
       " 'abuse .',\n",
       " 'abusive',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accommodation',\n",
       " 'according',\n",
       " 'account',\n",
       " 'account .',\n",
       " 'accuracy',\n",
       " 'accuracy response',\n",
       " 'act',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'action .',\n",
       " 'action wa',\n",
       " 'active',\n",
       " 'activity',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'add information',\n",
       " 'added',\n",
       " 'addiction',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'adult',\n",
       " 'adult ,',\n",
       " 'adult .',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advice',\n",
       " 'advice ,',\n",
       " 'advice .',\n",
       " 'advise',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'age',\n",
       " 'age .',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'aggressive',\n",
       " 'ago',\n",
       " 'ago ,',\n",
       " 'ago .',\n",
       " 'agree',\n",
       " 'agree .',\n",
       " 'agreed',\n",
       " 'agreement',\n",
       " 'agreement ,',\n",
       " 'ahead',\n",
       " 'air',\n",
       " 'alcohol',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'alright',\n",
       " 'amazing',\n",
       " 'american',\n",
       " 'amp',\n",
       " 'amp ;',\n",
       " 'and/or',\n",
       " 'anger',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'answer',\n",
       " 'answer ,',\n",
       " 'answer .',\n",
       " 'answer question',\n",
       " 'answer regarding',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anymore',\n",
       " 'anymore .',\n",
       " 'anyways',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apartment .',\n",
       " 'apologize',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appears',\n",
       " 'appears forgot',\n",
       " 'application',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'apply question',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'area',\n",
       " 'aren',\n",
       " 'aren ',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'arm',\n",
       " 'arrangement',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'article',\n",
       " 'asap',\n",
       " 'asap .',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'ask ,',\n",
       " 'ask .',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'assault',\n",
       " 'assaulted',\n",
       " 'asset',\n",
       " 'asshole',\n",
       " 'assume',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attorney',\n",
       " 'attorney ,',\n",
       " 'attorney .',\n",
       " 'attracted',\n",
       " 'attractive',\n",
       " 'author',\n",
       " 'author :',\n",
       " 'authority',\n",
       " 'automatically',\n",
       " 'automatically .',\n",
       " 'available',\n",
       " 'avoid',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'away ,',\n",
       " 'away .',\n",
       " 'awful',\n",
       " 'awkward',\n",
       " 'b',\n",
       " 'b )',\n",
       " 'baby',\n",
       " 'background',\n",
       " 'bad',\n",
       " 'bad .',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'balance',\n",
       " 'ball',\n",
       " 'bank',\n",
       " 'bank account',\n",
       " 'bar',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'bathroom',\n",
       " 'bc',\n",
       " 'beat',\n",
       " 'bed',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'behavior',\n",
       " 'behavior .',\n",
       " 'behaviour',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'believe .',\n",
       " 'believe wa',\n",
       " 'benefit',\n",
       " 'best',\n",
       " 'best .',\n",
       " 'best friend',\n",
       " 'best luck',\n",
       " 'best thing',\n",
       " 'best way',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'better ,',\n",
       " 'better .',\n",
       " 'bf',\n",
       " 'big',\n",
       " 'big deal',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bike',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bit .',\n",
       " 'bitch',\n",
       " 'black',\n",
       " 'blame',\n",
       " 'block',\n",
       " 'blood',\n",
       " 'blow',\n",
       " 'board',\n",
       " 'bob',\n",
       " 'body',\n",
       " 'body original',\n",
       " 'body post',\n",
       " 'bond',\n",
       " 'book',\n",
       " 'born',\n",
       " 'bos',\n",
       " 'bot',\n",
       " 'bot ,',\n",
       " 'bot sole',\n",
       " 'bother',\n",
       " 'bought',\n",
       " 'boundary',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'boyfriend .',\n",
       " 'brain',\n",
       " 'breach',\n",
       " 'break',\n",
       " 'break ,',\n",
       " 'break .',\n",
       " 'breaking',\n",
       " 'breakup',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'buddy',\n",
       " 'build',\n",
       " 'building',\n",
       " 'bullshit',\n",
       " 'bunch',\n",
       " 'business',\n",
       " 'business .',\n",
       " 'busy',\n",
       " 'buy',\n",
       " 'buyer',\n",
       " 'buying',\n",
       " 'ca',\n",
       " \"ca n't\",\n",
       " 'california',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calm',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'canada',\n",
       " 'cancel',\n",
       " 'car',\n",
       " 'car ,',\n",
       " 'car .',\n",
       " 'card',\n",
       " 'card .',\n",
       " 'care',\n",
       " 'care ,',\n",
       " 'care .',\n",
       " 'career',\n",
       " 'careful',\n",
       " 'caring',\n",
       " 'carrier',\n",
       " 'carry',\n",
       " 'case',\n",
       " 'case ,',\n",
       " 'case .',\n",
       " 'cash',\n",
       " 'casual',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'certificate',\n",
       " 'certified',\n",
       " 'chance',\n",
       " 'chance .',\n",
       " 'change',\n",
       " 'change ,',\n",
       " 'change .',\n",
       " 'changed',\n",
       " 'changing',\n",
       " 'character',\n",
       " 'charge',\n",
       " 'charge .',\n",
       " 'charged',\n",
       " 'chat',\n",
       " 'cheat',\n",
       " 'cheated',\n",
       " 'cheated .',\n",
       " 'cheater',\n",
       " 'cheating',\n",
       " 'cheating ,',\n",
       " 'cheating .',\n",
       " 'check',\n",
       " 'check .',\n",
       " 'checked',\n",
       " 'child',\n",
       " 'child ,',\n",
       " 'child .',\n",
       " 'child support',\n",
       " 'choice',\n",
       " 'choice .',\n",
       " 'choose',\n",
       " 'chose',\n",
       " 'christian',\n",
       " 'circumstance',\n",
       " 'citizen',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'claim .',\n",
       " 'claim court',\n",
       " 'claiming',\n",
       " 'class',\n",
       " 'clause',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clear .',\n",
       " 'clearly',\n",
       " 'client',\n",
       " 'close',\n",
       " 'closure',\n",
       " 'clothes',\n",
       " 'club',\n",
       " 'clue',\n",
       " 'code',\n",
       " 'cold',\n",
       " 'collect',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'come',\n",
       " 'come ,',\n",
       " 'come .',\n",
       " 'comfortable',\n",
       " 'coming',\n",
       " 'coming soon',\n",
       " 'comment',\n",
       " 'comment ,',\n",
       " 'comment .',\n",
       " 'comment ha',\n",
       " 'commenting',\n",
       " 'commenting rule',\n",
       " 'commit',\n",
       " 'committed',\n",
       " 'common',\n",
       " 'common question',\n",
       " 'communicate',\n",
       " 'communication',\n",
       " 'community',\n",
       " 'company',\n",
       " 'company ,',\n",
       " 'company .',\n",
       " 'complain',\n",
       " 'complaint',\n",
       " 'complaint .',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'complicated',\n",
       " 'comply',\n",
       " 'comply rule',\n",
       " 'compromise',\n",
       " 'computer',\n",
       " 'concern',\n",
       " 'concern .',\n",
       " 'concerned',\n",
       " 'condition',\n",
       " 'confidence',\n",
       " 'confident',\n",
       " 'confirm',\n",
       " 'confront',\n",
       " 'confused',\n",
       " 'connection',\n",
       " 'consent',\n",
       " 'consequence',\n",
       " 'consider',\n",
       " 'consideration',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'constantly',\n",
       " 'consult',\n",
       " 'consultation',\n",
       " 'contact',\n",
       " 'contact .',\n",
       " 'contact moderator',\n",
       " 'contacted',\n",
       " 'contacting',\n",
       " 'contain',\n",
       " 'content',\n",
       " 'context',\n",
       " 'continue',\n",
       " 'contract',\n",
       " 'contract .',\n",
       " 'contractor',\n",
       " 'control',\n",
       " 'control .',\n",
       " 'controlling',\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr3.best_estimator_.named_steps['cv'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:40:57.730327Z",
     "start_time": "2019-04-08T15:40:29.039633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9172132713494237\n",
      "0.8464183381088826\n"
     ]
    }
   ],
   "source": [
    "print(gs_lr3.score(X_train_com, y_train_com))\n",
    "\n",
    "print(gs_lr3.score(X_test_com, y_test_com))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:40:57.749097Z",
     "start_time": "2019-04-08T15:40:57.735455Z"
    }
   },
   "outputs": [],
   "source": [
    "## HashingVectorizer performance test\n",
    "\n",
    "pipe_hv = Pipeline([\n",
    "    ('hv', HashingVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params_hv = {\n",
    "    'hv__stop_words' : [None, 'english'],\n",
    "    'hv__ngram_range' : [(1,2)],\n",
    "#    'lr__solver' : ['newton-cg'],\n",
    "    'lr__penalty' : ['l2'],\n",
    "    'lr__C': [.5, 1]\n",
    "}\n",
    "\n",
    "gs_hv = GridSearchCV(pipe_hv, param_grid=pipe_params_hv, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:40:57.764888Z",
     "start_time": "2019-04-08T15:40:57.755126Z"
    }
   },
   "outputs": [],
   "source": [
    "##TfidfVectorizer performance test\n",
    "pipe_tfdf = Pipeline([\n",
    "    ('tf', TfidfVectorizer(tokenizer=StemTokenizer())),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params_tf = {\n",
    "    'tf__stop_words' : ['english'],\n",
    "    'tf__max_features': [3000],\n",
    "    'tf__min_df': [2],\n",
    "    'tf__max_df': [.9],\n",
    "}\n",
    "\n",
    "gs_tf = GridSearchCV(pipe_tfdf, param_grid=pipe_params_tf, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:52:35.954967Z",
     "start_time": "2019-04-08T15:40:57.769644Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'tf__stop_words': ['english'], 'tf__max_features': [3000], 'tf__min_df': [2], 'tf__max_df': [0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tf.fit(X_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:52:35.982938Z",
     "start_time": "2019-04-08T15:52:35.958922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=3000, min_df=2,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:56:26.315797Z",
     "start_time": "2019-04-08T15:52:35.985869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7919505826912056\n",
      "0.7799426934097421\n",
      "0.9807740117676307\n",
      "0.9704200845140443\n"
     ]
    }
   ],
   "source": [
    "#Print Comment Results\n",
    "print(gs_tf.score(X_train_com, y_train_com))\n",
    "print(gs_tf.score(X_test_com, y_test_com))\n",
    "\n",
    "#Print Submission Results\n",
    "print(gs_tf.score(X_train_sub, y_train_sub))\n",
    "print(gs_tf.score(X_test_sub, y_test_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T15:56:26.329987Z",
     "start_time": "2019-04-08T15:56:26.319351Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier(n_jobs=-1))\n",
    "])\n",
    "    \n",
    "rf_params = {\n",
    "    'cv__ngram_range' : [(1,2)],\n",
    "    'rf__n_estimators': [10, 20, 30],\n",
    "    'rf__max_depth': [None, 1, 2, 3, 4, 5],\n",
    "    'rf__min_samples_split': [2,3,4]\n",
    "    \n",
    "}\n",
    "\n",
    "gs_rf = GridSearchCV(pipe_rf, param_grid=rf_params, cv=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T16:49:52.798163Z",
     "start_time": "2019-04-08T15:56:26.333766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_a..._jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'cv__ngram_range': [(1, 2)], 'rf__n_estimators': [10, 20, 30], 'rf__max_depth': [None, 1, 2, 3, 4, 5], 'rf__min_samples_split': [2, 3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.fit(X_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T16:49:52.814855Z",
     "start_time": "2019-04-08T16:49:52.803574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv__ngram_range': (1, 2),\n",
       " 'rf__max_depth': None,\n",
       " 'rf__min_samples_split': 2,\n",
       " 'rf__n_estimators': 30}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T16:50:13.105360Z",
     "start_time": "2019-04-08T16:49:52.818976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931159651022097\n",
      "0.6792741165234002\n",
      "0.9999171293610674\n",
      "0.9415858811831966\n"
     ]
    }
   ],
   "source": [
    "#Print Comment Results\n",
    "print(gs_rf.score(X_train_com, y_train_com))\n",
    "print(gs_rf.score(X_test_com, y_test_com))\n",
    "\n",
    "#Print Submission results\n",
    "print(gs_rf.score(X_train_sub, y_train_sub))\n",
    "print(gs_rf.score(X_test_sub, y_test_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Even with already high scores in the other models, we just wanted to test out the capabilities and the tuning of the XGB classifier as it pertains to our classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T16:50:13.120912Z",
     "start_time": "2019-04-08T16:50:13.108804Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_boost = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('xg', XGBClassifier(objective='binary:logistic',seed=42))\n",
    "])\n",
    "    \n",
    "boost_params = {\n",
    "    'xg__n_estimators=': [1000],\n",
    "    'xg__learning_rate': [.1],\n",
    "    'xg__max_depth':[11],\n",
    "    'xg__min_child_weight': [3],\n",
    "    'xg__gamma': [0],\n",
    "    'xg__subsample':[.8],\n",
    "    'xg__colsample_bytree':[.8],\n",
    "    'xg__pos_weight':[1],\n",
    "    'xg__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gs_boost = GridSearchCV(pipe_boost, param_grid=boost_params,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T17:06:10.449286Z",
     "start_time": "2019-04-08T16:50:13.133170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_a...0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n",
       "       subsample=1))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'xg__n_estimators=': [1000], 'xg__learning_rate': [0.1], 'xg__max_depth': [11], 'xg__min_child_weight': [3], 'xg__gamma': [0], 'xg__subsample': [0.8], 'xg__colsample_bytree': [0.8], 'xg__pos_weight': [1], 'xg__reg_alpha': [1e-05, 0.01, 0.1, 1, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_boost.fit(X_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T17:06:10.474573Z",
     "start_time": "2019-04-08T17:06:10.456451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xg__colsample_bytree': 0.8,\n",
       " 'xg__gamma': 0,\n",
       " 'xg__learning_rate': 0.1,\n",
       " 'xg__max_depth': 11,\n",
       " 'xg__min_child_weight': 3,\n",
       " 'xg__n_estimators=': 1000,\n",
       " 'xg__pos_weight': 1,\n",
       " 'xg__reg_alpha': 1e-05,\n",
       " 'xg__subsample': 0.8}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_boost.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T17:06:12.795829Z",
     "start_time": "2019-04-08T17:06:10.478423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7712539005285615\n",
      "0.7619866284622732\n"
     ]
    }
   ],
   "source": [
    "print(gs_boost.score(X_train_com,y_train_com))\n",
    "\n",
    "print(gs_boost.score(X_test_com,y_test_com))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T17:06:19.196182Z",
     "start_time": "2019-04-08T17:06:12.798514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9938675727189856\n",
      "0.9684315187670892\n"
     ]
    }
   ],
   "source": [
    "print(gs_boost.score(X_train_sub,y_train_sub))\n",
    "\n",
    "print(gs_boost.score(X_test_sub,y_test_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T23:10:31.444843Z",
     "start_time": "2019-04-08T23:10:31.175753Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gs_boost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f3377fa74f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'I am a lawyer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#A random string simulating a post\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgs_boost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_post\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Using any of above gridsearchedpipelines to test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#efficiacy, where a 0 will be s1 and 1 will be s2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gs_boost' is not defined"
     ]
    }
   ],
   "source": [
    "test_post = ['I am a lawyer'] #A random string simulating a post\n",
    "gs_boost.predict(test_post) #Using any of above gridsearchedpipelines to test \n",
    "#efficiacy, where a 0 will be s1 and 1 will be s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Gathering roughly 8,000 posts and 10,000 posts from each subreddit, the cleaning process led to very little loss in of data and as such, each subreddit was well-represented in the final classification model. No balancing of classes was required.\n",
    "\n",
    "From there, analysis and optimization of grid searches over hyperparameters led to the following insights. From all NLP models -  Logistic Regression, Multinomial Naive Bayes, Random Forest Classification, and XGBoost, it seemed like in general the Logistic Regression performed the best with the highest interpretability and least tuning.\n",
    "\n",
    "A simple grid search on the Logistic Regression parameters while it took several iterations, was much simpler than the fine tuning that a XGBoost required. It was simply much easier to get a good result with Logistic Regression. Results of training off the submission selftext typically scored nearly 0.99 and roughly 0.95/0.96 across most of the model. Understandably so, the use of XGBoost as the last model was extraneous, as there was not much room for improvement, and our results show that as such. XGBoost's fitting did not have room to perform much better. Given it's notable efficacy within the Machine Learning community, it is not hard to imagine that with even more tuning of the hyperparameters, XGBoost could beat all the other previously mentioned models. However, in this case, our subreddits were farther apart in terms of language than we initially estimated and NLP should be able to distinguish them close to 100% of the time. Most of the language we found that scored highly was very indicative of r/legaladvice.\n",
    "\n",
    "One extra note of insight we found was that training models on comments alone and lead to good post accuracy scores (~.99). However, training a model on selftext could only lead to scores in the neighborhood of (0.80) for identifying the subreddit given a comment. This suggests that selftexts are MORE indicative of a subreddit than the comments. And intuitively, we think this is true because there are many more comments than there are posts (most users comment, but few post) and these comments may come with much more variance than the possibly moderate posts. We may also theorize that posts are typically submitted by the most dedicated members of the subreddit and as such, are more representative of the community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "    - www.reddit.com/r/legaladvice\n",
    "    - www.reddit.com/r/relationship_advice\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
